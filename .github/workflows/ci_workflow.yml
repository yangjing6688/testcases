name: CI Workflow
# Ensures only one instance of the CI workflow is running per branch
# This stops multiple rapid commits from holding up the process
concurrency: ci-${{ github.ref }}

on:
  workflow_dispatch:

  pull_request:
    branches: [main, RELEASE-**, 2022_Q1r2]

env:
  pytest_func_path:             pytest_func_tests.txt
  pytest_non_prod_path:         pytest_non_prod_tests.txt
  pytest_libs_path:             pytest_libs.txt
  pytest_res:                   pytest_resources.txt
  robot_func_path:              robot_func_tests.txt
  robot_non_prod_path:          robot_non_prod_tests.txt
  robot_libs_path:              robot_libs.txt
  testbed_prod_path:            testbeds_prod.txt
  testbed_non_prod_path:        testbeds_non_prod.txt
  other_files_path:             other_files.txt
  changed_files_artifact_name:  changed-files
  testbed_files_report_path:    testbed_files_report.txt
  testcase_files_report_path:   testcase_files_report.txt
  directory_struct_report_path: directory_structure_report.txt
  testcase_naming_report_path:  testcase_naming_report.txt
  tag_marker_report_path:       tag_and_marker_report.txt
  # Commented out on 4/21/22 by petersadej.
  # This report is currently broken due to changes outside of this script. All or parts of this can be used in the future
  # testcase_func_report_path:    testcase_functionality_report.txt
  report_artifact_name:         CI-reports-${{ github.event.repository.name }}-run-${{ github.run_number }}
  documentation_url:            https://extremenetworks2com.sharepoint.com/:w:/r/sites/qa-extauto/_layouts/15/Doc.aspx?sourcedoc=%7B357F1F2C-B274-4739-9D87-A67A72875846%7D&file=Automation%20Process%20-%20Draft.docx

jobs:
  check_changes:
    name: Check for changes in test scripts
    runs-on: ubuntu-latest
    # These outputs are used to determine whether certain checks should run
    outputs:
      run-pytest: ${{ steps.pytest-func.outputs.any_changed }}
      run-robot: ${{ steps.robot-func.outputs.any_changed }}

    steps:
    - name: Checkout
      uses: actions/checkout@v2

    # https://github.com/tj-actions/changed-files
    - name: Get changed pytest functional tests
      id: pytest-func
      uses: tj-actions/changed-files@v12.2
      with:
        files: |
          Tests/Pytest/(SystemTest|Functional)/([^/]+/)+TestCases/
        separator: ','

    - name: Get changed pytest non-prod tests
      id: pytest-non-prod
      uses: tj-actions/changed-files@v12.2
      with:
        files: |
          Tests/Pytest/NonProduction/([^/]+/)+TestCases/
        separator: ','

    - name: Get changed pytest resources
      id: pytest-resources
      uses: tj-actions/changed-files@v12.2
      with:
        files: |
          Tests/Pytest/([^/]+/)+Resources/
        separator: ','

    - name: Get changed robot functional tests
      id: robot-func
      uses: tj-actions/changed-files@v12.2
      with:
        files: |
          Tests/Robot/(SystemTest|Functional)/([^/]+/)+TestCases/
        separator: ','

    - name: Get changed robot non-prod tests
      id: robot-non-prod
      uses: tj-actions/changed-files@v12.2
      with:
        files: |
          Tests/Robot/NonProduction/([^/]+/)+TestCases/
        separator: ','

    - name: Get changed robot libraries
      id: robot-libs
      uses: tj-actions/changed-files@v12.2
      with:
        files: |
          Tests/Robot/Libraries/([^/]+/)+
        separator: ','

    - name: Get changed pytest libraries
      id: pytest-libs
      uses: tj-actions/changed-files@v12.2
      with:
        files: |
          Tests/Pytest/Libraries/([^/]+/)+
        separator: ','

    - name: Get changed Non-Prod Testbed YAML files
      id: testbed-non-prod
      uses: tj-actions/changed-files@v12.2
      with:
        files: |
          TestBeds/[^/]+/(Demo|Dev)/([^/]+/)*.+\.ya?ml
          TestBeds/Templates/([^/]+/)*.+\.ya?ml
        separator: ','

    - name: Get changed Prod Testbed YAML files
      id: testbed-prod
      uses: tj-actions/changed-files@v12.2
      with:
        files: |
          TestBeds/[^/]+/(Prod|SystemsTest)/([^/]+/)*.+\.ya?ml
        separator: ','

    # Outputs a list of files breaking directory rules that we do and don't care about
    - name: Get other changed files
      id: other-files
      uses: tj-actions/changed-files@v12.2
      with:
        files: |
          Tests/(Pytest|Robot)/(SystemTest|Functional|NonProduction)/([^/]+/)+(TestCases|Resources)/
          Tests/(Pytest|Robot)/Libraries/([^/]+/)+
          Tests/(Pytest|Robot)/Demos/
          __init__.py
          .pylintrc
          [rR][eE][aA][dD][mM][eE]\.md
        separator: ','

    # Outputs a list of files breaking directory rules that we don't care about
    - name: Get other changed files we dont care about
      id: other-files2
      uses: tj-actions/changed-files@v12.2
      with:
        files: |
          Tests
        separator: ','

    # Diff two lists to get just the files we care about
    # All files in this list need to conform to directory structure rules
    - name: Diff other files lists
      shell: python
      run: |
        l1 = "${{ steps.other-files.outputs.other_changed_files }}".split(",")
        l2 = "${{ steps.other-files2.outputs.other_changed_files }}".split(",")
        diff_list = [x for x in l1 if x not in l2]
        diff_string = ",".join(diff_list)
        print(diff_string)
        with open('${{ env.other_files_path }}', 'w') as outfile:
          outfile.write(diff_string)


    - name: List all modified files
      env:
        PY_FUNC:          ${{ steps.pytest-func.outputs.all_changed_files }}
        PY_NON_PROD:      ${{ steps.pytest-non-prod.outputs.all_changed_files }}
        PY_LIBS:          ${{ steps.pytest-libs.outputs.all_changed_files }}
        PY_RES:           ${{ steps.pytest-resources.outputs.all_changed_files }}
        ROBOT_FUNC:       ${{ steps.robot-func.outputs.all_changed_files }}
        ROBOT_NON_PROD:   ${{ steps.robot-non-prod.outputs.all_changed_files }}
        ROBOT_LIBS:       ${{ steps.robot-libs.outputs.all_changed_files }}
        TESTBED_PROD:     ${{ steps.testbed-prod.outputs.all_changed_files }}
        TESTBED_NON_PROD: ${{ steps.testbed-non-prod.outputs.all_changed_files }}
        OTHER_FILES:      ${{ steps.other-files.outputs.other_changed_files }}
      run: |
        echo $PY_FUNC > ${{ env.pytest_func_path }}
        echo $PY_NON_PROD > ${{ env.pytest_non_prod_path }}
        echo $PY_LIBS > ${{ env.pytest_libs_path }}
        echo $PY_RES > ${{ env.pytest_res }}
        echo $ROBOT_FUNC > ${{ env.robot_func_path }}
        echo $ROBOT_NON_PROD > ${{ env.robot_non_prod_path }}
        echo $ROBOT_LIBS > ${{ env.robot_libs_path }}
        echo $TESTBED_PROD > ${{ env.testbed_prod_path }}
        echo $TESTBED_NON_PROD > ${{ env.testbed_non_prod_path }}
        # echo $OTHER_FILES > ${{ env.other_files_path }}

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v2
      with:
        name: ${{ env.changed_files_artifact_name }}
        path: |
          ${{ env.pytest_func_path }}
          ${{ env.pytest_non_prod_path }}
          ${{ env.pytest_libs_path }}
          ${{ env.pytest_res }}
          ${{ env.robot_func_path }}
          ${{ env.robot_non_prod_path }}
          ${{ env.robot_libs_path }}
          ${{ env.testbed_prod_path }}
          ${{ env.testbed_non_prod_path }}
          ${{ env.other_files_path }}

  check_testbeds:
    needs: [check_changes]
    name: Testbed file validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}

    steps:
    - name: Checkout
      uses: actions/checkout@v2

    - uses: actions/download-artifact@v2
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        # Create report file
        cat >${{ env.testbed_files_report_path }} <<EOL
        ******************************************************
        ************** Testbed YAML File Report **************
        ******************************************************

        [*] Checking production testbed files...

        EOL

    - name: Handle Skip Requests
      run: |
        python .github/workflows/skip_handler.py --actor "${{ github.actor }}" --title '${{ github.event.pull_request.title }}' --check testbed | tee -a ${{ env.testbed_files_report_path }}

    - name: Check production test bed files
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        set -o pipefail # Preserves the return code from the python script in pipeline below
        python .github/workflows/testbed_yaml_parser.py ${{ env.testbed_prod_path }} | tee -a ${{ env.testbed_files_report_path }}

    - name: Check non-production test bed files
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        printf '%s\n' "[*] Checking non-production testbed files..."
        python .github/workflows/testbed_yaml_parser.py ${{ env.testbed_non_prod_path }} | tee -a ${{ env.testbed_files_report_path }}

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v2
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.testbed_files_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "::set-output name=status::${{ job.status }}"

  check_file:
    needs: [check_changes]
    name: File name and location validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}

    steps:
    - uses: actions/download-artifact@v2
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        # Create report file
        cat >${{ env.testcase_files_report_path }} <<EOL
        *****************************************************
        ********** Robot/PyTest file Naming Report **********
        *****************************************************

        EOL

    - name: Handle Skip Requests
      run: |
        python .github/workflows/skip_handler.py --actor "${{ github.actor }}" --title '${{ github.event.pull_request.title }}' --check file | tee -a ${{ env.testcase_files_report_path }}

    - name: Process Pytest Production file(s)
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for path in $( cat ${{ env.pytest_func_path }} | sed -e 's/ //g' ); do
          # Only allow __init__.py or files that conform to "test_<testname>_<valid_jira_id>.py"
          # valid jira id: One or more letters followed by one or more numbers. A dash between the letters and numbers is optional.
          # This can potentially be followed by _ and 1 or more numbers if multiple testcase files are needed for the same id.
          # Examples: test_blah_app-45   test_blah_blah_app45_1   test_blah_app-45_1   test_blah_app45
          # Note: inside bash [[ ]] you need to use extended regex format

          errors=0
          f=$( basename "$path" )
          let "files_tested += 1"
          printf "[*] Processing Pytest file: $path ...\n" | tee -a ${{ env.testcase_files_report_path }}

          # Test for .py file extension
          if [[ ! $f =~ .*\.py$ ]] ; then
              printf "[**] FAIL: $path is NOT a Pytest file!  Only .py files are allowed in this directory.\n" | tee -a ${{ env.testcase_files_report_path }}
              rc=1; let "errors += 1"
          fi

          # Test for "test_" prefix. Ignore __init__.py, conftest.py, .pylintrc files
          if [[ ! $f =~ ^test_.*\.py$|__init__\.py$|conftest\.py$|\.pylintrc$ ]] ; then
              echo "[**] FAIL: $path is missing the \"test_\" prefix!" | tee -a ${{ env.testcase_files_report_path }}
              rc=1; let "errors += 1"
          fi

          # Check for valid Jira ID suffix. Ignore __init__.py, conftest.py, .pylintrc files
          if [[ ! $f =~ ^test_.*_[a-zA-Z]+[\-_]?[0-9]+(_[0-9]+)?\.py$|__init__\.py$|conftest\.py$|\.pylintrc$ ]] ; then
              echo "[**] FAIL: $path does not contain a valid Jira ID suffix!" | tee -a ${{ env.testcase_files_report_path }}
              rc=1; let "errors += 1"
          fi

          if (( errors == 0 )); then
              printf "[**] PASS: $path\n" | tee -a ${{ env.testcase_files_report_path }}
          else
              echo '[**] For more information see the Test Suite File Name section of ${{ env.documentation_url }}' | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          echo "[*] No Pytest Production files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Process Pytest Non-Production file(s)
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for path in $( cat ${{ env.pytest_non_prod_path }} | sed -e 's/ //g' ); do
          errors=0
          f=$( basename "$path" )
          let "files_tested += 1"
          printf "[*] Processing Pytest file: $path ...\n" | tee -a ${{ env.testcase_files_report_path }}
          # Test for .py file extension
          if [[ ! $f =~ .*\.py$ ]] ; then
              printf "[**] FAIL: $path is NOT a Pytest file! Only .py files are allowed in this directory.\n" | tee -a ${{ env.testcase_files_report_path }}
              rc=1; let "errors += 1"
          fi

          # Only allow files that conform to "test_<testname>.py". Ignore __init__.py, conftest.py, .pylintrc files
          # Note: inside bash [[ ]] you need to use extended regex format
          if [[ ! "$f" =~ ^test_.*\.py$|__init__\.py$|conftest\.py$|\.pylintrc$ ]] ; then
              echo "[**] FAIL: $path is missing the \"test_\" prefix!" | tee -a ${{ env.testcase_files_report_path }}
              rc=1; let "errors += 1"
          fi

          if (( errors == 0 )); then
              printf "[**] PASS: $path\n" | tee -a ${{ env.testcase_files_report_path }}
          else
              echo '[**] For more information see the Test Suite File Name section of ${{ env.documentation_url }}' | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          echo "[*] No Pytest Non-Production files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Process Pytest Library file(s)
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for path in $( cat ${{ env.pytest_libs_path }} | sed -e 's/ //g' ); do
          f=$( basename "$path" )
          let "files_tested += 1"
          printf "[*] Processing Python file: $path ...\n" | tee -a ${{ env.testcase_files_report_path }}
          if [[ ! $f =~ .*\.py$ ]] ; then
              printf "[**] FAIL: $path is NOT a Python file! Only .py files are allowed in this directory.\n" | tee -a ${{ env.testcase_files_report_path }}
              rc=1;
          else
              printf "[**] PASS: $path\n" | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          echo "[*] No Pytest Library files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Process Robot Production file(s)
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for path in $( cat ${{ env.robot_func_path }} | sed -e 's/ //g' ); do
          # Only allow files that conform to "<testname>_<valid_jira_id>.robot" or "__init__.robot"
          # See "Process Pytest Production file(s)" for definition of <valid_jira_id>

          errors=0
          f=$( basename "$path" )
          let "files_tested += 1"
          printf "[*] Processing Robot file: $path ...\n" | tee -a ${{ env.testcase_files_report_path }}

          # Test for .robot file extension
          if [[ ! $f =~ .*\.robot$ ]] ; then
              printf "[**] FAIL: $path is NOT a Robot file! Only .robot files are allowed in this directory.\n" | tee -a ${{ env.testcase_files_report_path }}
              rc=1; let "errors += 1"
          fi

          # Check for Jira ID suffix. Ignore __init__.robot files
          if [[ ! $f =~ _[a-zA-Z]+[\-_]?[0-9]+(_[0-9]+)?\.robot$|__init__\.robot$ ]] ; then
              echo "[**] FAIL: $path does not contain a valid Jira ID suffix!" | tee -a ${{ env.testcase_files_report_path }}
              rc=1; let "errors += 1"
          fi

          if (( errors == 0 )); then
              printf "[**] PASS: $path\n" | tee -a ${{ env.testcase_files_report_path }}
          else
              echo '[**] For more information see the Test Suite File Name section of ${{ env.documentation_url }}' | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          echo "[*] No Robot Production files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Process Robot Non-Production file(s)
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for path in $( cat ${{ env.robot_non_prod_path }} | sed -e 's/ //g' ); do
          f=$( basename "$path" )
          let "files_tested += 1"
          printf "[*] Processing Robot file: $path ...\n" | tee -a ${{ env.testcase_files_report_path }}
          if [[ ! $f =~ .*\.robot$ ]] ; then
              printf "[**] FAIL: $path is NOT a Robot file! Only .robot files are allowed in this directory.\n" | tee -a ${{ env.testcase_files_report_path }}
              rc=1
          else
              printf "[**] PASS: $path\n" | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          echo "[*] No Robot Non-Production files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Process Robot Library file(s)
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for path in $( cat ${{ env.robot_libs_path }} | sed -e 's/ //g' ); do
          f=$( basename "$path" )
          let "files_tested += 1"
          printf "[*] Processing Robot file: $path ...\n" | tee -a ${{ env.testcase_files_report_path }}
          if [[ ! $f =~ .*\.robot$ ]] ; then
              printf "[**] FAIL: $path is NOT a Robot file! Only .robot files are allowed in this directory.\n" | tee -a ${{ env.testcase_files_report_path }}
              rc=1
          else
              printf "[**] PASS: $path\n" | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          echo "[*] No Robot Library files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v2
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.testcase_files_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "::set-output name=status::${{ job.status }}"

  check_dir:
    needs: [check_changes]
    name: Directory structure validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}

    steps:
    - name: Checkout
      uses: actions/checkout@v2

    - uses: actions/download-artifact@v2
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        # Create report file
        cat >${{ env.directory_struct_report_path }} <<EOL
        ********************************************************
        ************** Directory Structure Report **************
        ********************************************************

        EOL

    - name: Handle Skip Requests
      run: |
        python .github/workflows/skip_handler.py --actor "${{ github.actor }}" --title '${{ github.event.pull_request.title }}' --check dir | tee -a ${{ env.directory_struct_report_path }}

    - name: Check if any disallowed directories were created
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        trimmed_files=$(cat ${{ env.other_files_path }} | tr -d [:space:])
        if [[ $trimmed_files ]]; then
          echo "[*] FAIL: New directories and files not allowed in protected areas." | tee -a ${{ env.directory_struct_report_path }}
          echo "[*] Common reasons for seeing this error:" | tee -a ${{ env.directory_struct_report_path }}
          echo "[*]   - Adding files outside of TestCases or Resource directories. (Except: __init__.py, .pylintrc, readme.md)" | tee -a ${{ env.directory_struct_report_path }}
          echo "[*]   - Adding directories outisde of /Tests/<framework>/<test type>/ " | tee -a ${{ env.directory_struct_report_path }}
          echo '[*] For more information see the Test Suite Location section of ${{ env.documentation_url }}' | tee -a ${{ env.directory_struct_report_path }}
          echo "[*] Offending files/directories:" | tee -a ${{ env.directory_struct_report_path }}

          IFS=','
          for path in $trimmed_files ; do
            echo "[**] $path" | tee -a ${{ env.directory_struct_report_path }}
          done
          exit 1
        else
          echo "[*] PASS: No files/directories found in protected areas." | tee -a ${{ env.directory_struct_report_path }}
        fi

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v2
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.directory_struct_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "::set-output name=status::${{ job.status }}"

  check_names:
    needs: [check_file, check_dir]
    name: Testcase naming validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}

    steps:
    - name: Checkout modified files
      uses: actions/checkout@v2

    - uses: actions/download-artifact@v2
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        cat >${{ env.testcase_naming_report_path }} <<EOL
        ******************************************************
        *************** Testcase Naming Report ***************
        ******************************************************

        EOL

    # Checks for testcases being mistakenly put in the resources directory
    - name: Process modified Pytest Resources files
      run: |
        files_tested=0
        rc=0
        for path in $( cat ${{ env.pytest_res }} | tr ',' '\n' ) ; do
            filename=$( basename "$path" )
            let "files_tested += 1"
            echo "[*] Processing Pytest Resources file: $path ..." | tee -a ${{ env.testcase_naming_report_path }}

            # Skip __init__.py, conftest.py, .pylintrc files
            #
            if [[ ${filename} =~ __init__\.py$|conftest\.py$|\.pylintrc$ ]] ; then
                echo "[**] PASS - File: ${path}" | tee -a ${{ env.testcase_naming_report_path }}
                continue
            fi

            for func in $( grep -w def $path | awk '{print $2}' ); do
                echo "Testing Function: ${func} ..." | tee -a ${{ env.testcase_naming_report_path }}

                if [[ ${func} =~ ^test_.*$ ]] ; then
                  echo "[**] FAIL - Function: ${func}" | tee -a ${{ env.testcase_naming_report_path }}
                  rc=1
                else
                  echo "[**] PASS - Function: ${func}" | tee -a ${{ env.testcase_naming_report_path }}
                fi
            done
        done
        if (( files_tested == 0 )); then
          echo "[*] No Pytest Resource files found. Skipping these tests..." | tee -a ${{ env.testcase_naming_report_path }}
        fi
        exit $rc

    # Checks for non-testcases in testcase directory
    - name: Process modified Pytest Test files
      run: |
        files_tested=0
        rc=0
        for path in $( cat ${{ env.pytest_func_path }} ${{ env.pytest_non_prod_path }} | tr ',' '\n' ) ; do
            filename=$( basename "$path" )
            let "files_tested += 1"
            echo "[*] Processing Pytest Test file: $path ..." | tee -a ${{ env.testcase_naming_report_path }}

            # Skip __init__.py, conftest.py, .pylintrc files
            #
            if [[ ${filename} =~ __init__\.py$|conftest\.py$|\.pylintrc$ ]] ; then
                echo "[**] PASS - File: ${path}" | tee -a ${{ env.testcase_naming_report_path }}
                continue
            fi

            func=$( grep -w def $path | awk '{print $2}' | grep ^test_ )
            if [[ ${func-} ]] ; then
              echo "[**] PASS - File: ${path}" | tee -a ${{ env.testcase_naming_report_path }}
            else
              echo "[**] FAIL - File: ${path}" | tee -a ${{ env.testcase_naming_report_path }}
              rc=1
            fi
        done
        if (( files_tested == 0 )); then
          echo "[*] No Pytest Test files found. Skipping these tests..." | tee -a ${{ env.testcase_naming_report_path }}
        fi
        exit $rc

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v2
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.testcase_naming_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "::set-output name=status::${{ job.status }}"

  check_tags:
    needs: [check_changes, check_names]
    name: Tags/Markers validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}
    env:
      pytest_result_file: pytest_data.json
      robot_result_file: robot_data.json

    steps:
    - uses: actions/download-artifact@v2
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        # Create report file
        cat >${{ env.tag_marker_report_path }} <<EOL
        ********************************************************
        ************** Testcase Tag/Marker Report **************
        ********************************************************

        EOL

        # pytest_files=$(cat ${{ env.pytest_func_path }} ${{ env.pytest_non_prod_path }} | tr ',\n' ' ')
        pytest_files=$(cat ${{ env.pytest_func_path }} | tr ',\n' ' ')

        # robot_files=$(cat ${{ env.robot_func_path }} ${{ env.robot_non_prod_path }} | tr ',\n' ' ')
        robot_files=$(cat ${{ env.robot_func_path }} | tr ',\n' ' ')

        # Set env vars
        echo "PYTEST_FILES=${pytest_files}" >> $GITHUB_ENV
        echo "ROBOT_FILES=${robot_files}" >> $GITHUB_ENV
        echo "RUN_PYTEST=${{ needs.check_changes.outputs.run-pytest }}" >> $GITHUB_ENV
        echo "RUN_ROBOT=${{ needs.check_changes.outputs.run-robot }}" >> $GITHUB_ENV

    # Only do the below steps if we need to evaluate any of the changes being committed
    - name: Checkout local repo
      if: ${{env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true'}}
      uses: actions/checkout@v2

    - name: Handle Skip Requests
      if: ${{env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true'}}
      run: |
        # Handle skipping entire Tags section
        #
        python .github/workflows/skip_handler.py --actor "${{ github.actor }}" --title '${{ github.event.pull_request.title }}' --check tags | tee -a ${{ env.tag_marker_report_path }}

        # Handle skipping entire only reserved tags checks (sanity, p1, p2, etc.)
        #
        python .github/workflows/skip_handler.py --actor "${{ github.actor }}" --title '${{ github.event.pull_request.title }}' \
            --check "reserved tags" --extended_auth "tags" --envvar "SKIP_RESERVED_TAGS" | tee -a ${{ env.tag_marker_report_path }}

    - name: Checkout framework
      if: ${{env.SKIP_CHECK == 'false' && (env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true')}}
      uses: actions/checkout@v2
      with:
        repository: extremenetworks/extreme_automation_framework
        token: ${{ secrets.REGISTRY_PAT }}
        path: extreme_automation_framework
        ref: main

    - uses: actions/download-artifact@v2
      if: ${{env.SKIP_CHECK == 'false' && (env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true')}}
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Install python requirements
      if: ${{env.SKIP_CHECK == 'false' && (env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true')}}
      run: |
        pip install -r requirements.txt

    # TODO: Figure out how to get pytest to only operate on input files. If that isn't viable
    #       run for all files and only grab the ones we want(would take SIGNIFICANTLY more time)
    # potential workaround: copy modified tests to another directory and run pytest there
    # this would require either making a whole mimiced dir tree or using a different method of identifying the tests
    - name: Run pytest inventory tool
      if: ${{env.SKIP_CHECK == 'false' && env.RUN_PYTEST == 'true'}}
      run: |
        # Add framework to PYTHONPATH
        export PYTHONPATH="${PYTHONPATH}:${PWD}/extreme_automation_framework"

        # Run pytest
        pytest --get_test_info cicd ${{ env.PYTEST_FILES }} || true

        # Check if inventory tool had problems
        if [[ ! -f "${{ env.pytest_result_file }}" ]]; then
            echo "Error: output file not found. Check above exception."
            exit 1
        fi

    - name: Run robot inventory tool
      if: ${{env.SKIP_CHECK == 'false' && env.RUN_ROBOT == 'true'}}
      env:
        ROBOT_TOOL_PATH: Tests/Robot/get_test_info.py
      run: |
        # Add framework to PYTHONPATH
        export PYTHONPATH="${PYTHONPATH}:${PWD}/extreme_automation_framework"

        # Run robot
        python $ROBOT_TOOL_PATH ${{ env.ROBOT_FILES }}

        # Check if inventory tool had problems
        if [[ ! -f "${{ env.robot_result_file }}" ]]; then
            echo "Error: output file not found. Check above exception."
            exit 1
        fi

    # Tags and Markers:
    # Tests should contain Tags (Robot) or Markers (pyTest) to categorize the tests allowing test case selection during a test run.  This allows the automation system to select or omit specific tests.  For example, if tests are specific to a particular device and cannot be run on other devices you would want to add a tag or marker to identify the hardware the test can run on.   The number and types of tags are essentially limitless.  To avoid clutter please review existing tags and reuse them where possible.
    #
    # The following tags will be selected by the automation team, test developers must avoid using them (CI/CD will check for this):
    # production, regression, nightly, sanity, p1, p2, p3, p4
    #
    # All tags or markers should be in lower case only.
    # Developers must have two tags or markers.  The first tag or marker will identify the qTest project and test case number in the format: <project>_tc_<test case number>.  The second tag or marker must be: “development”, indicating that this test case is new to the automation system.
    # The project names are short names that identify the qTest project.  The names and their matching project are listed below.
    #
    # Project Names
    # csit -> csit
    # xim -> XIQ Mainline
    # exos -> EXOS
    # voss -> VOSS
    # xiq -> xiq
    # xnt -> EXOS-NOS-TestPlans
    # xna -> EXOS-NOS-UnifiedAgile
    #
    #  Robot:
    # [Tags] csit_tc_4501    development
    #
    # PyTest:
    # @pytest.mark.csit_tc_4501
    # @pytest.mark.development
    #
    # Identifying the TestBed
    # The type of testbed must be specified in the test suite.  This should be done by specifying the testbed type using “Force Tags” in robot or by marking the class in PyTest.  Possible options for the testbed type are limited to:
    #
    # testbed_1_node
    # testbed_2_node
    # testbed_3_node
    # testbed_4_node
    # testbed_5_node
    #
    # Robot:
    # Robot uses “Force Tags” to mark all of the tests with the same tag.  The TestBed type must be identified in each Test Case.  Using “Force Tags” you can mark every test.  Here is an example:
    #
    # *** Settings ***
    # Force Tags      testbed_1_node
    #
    # PyTest:
    # To mark every test with the proper testbed you can mark the entire class with the testbed.  Here is an example:
    #
    # @mark.testbed_1_node
    # Class DefaultTests:
    #
    - name: Check pytest markers
      if: ${{env.SKIP_CHECK == 'false' && env.RUN_PYTEST == 'true'}}
      run: |
        set -o pipefail # Preserves the return code from the python script in pipeline below
        python .github/workflows/testcase_tags_parser.py ${{ env.pytest_result_file }} --mode validate_tags \
          --framework pytest --skip_reserved_tags ${SKIP_RESERVED_TAGS} --auth_token ${{ secrets.AUTOIQ_PAT }} \
          | tee -a ${{ env.tag_marker_report_path }}

    - name: Check robot tags
      if: ${{env.SKIP_CHECK == 'false' && env.RUN_ROBOT == 'true'}}
      run: |
        set -o pipefail # Preserves the return code from the python script in pipeline below
        python .github/workflows/testcase_tags_parser.py ${{ env.robot_result_file }} --mode validate_tags \
          --framework robot --skip_reserved_tags ${SKIP_RESERVED_TAGS} --auth_token ${{ secrets.AUTOIQ_PAT }} \
          | tee -a ${{ env.tag_marker_report_path }}

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v2
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.tag_marker_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "::set-output name=status::${{ job.status }}"

  check_tests_functionality:
    needs: [check_changes, check_tags]
    name: Test functionality validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}

    steps:
    - name: Checkout modified files
      uses: actions/checkout@v2

    - uses: actions/download-artifact@v2
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        echo "RUN_PYTEST=${{ needs.check_changes.outputs.run-pytest }}" >> $GITHUB_ENV
        echo "RUN_ROBOT=${{ needs.check_changes.outputs.run-robot }}" >> $GITHUB_ENV

    # Commented out on 4/21/22 by petersadej.
    # This report is currently broken due to changes outside of this script. All or parts of this can be used in the future
    #
    #     # Create report file
    #     cat >${{ env.testcase_func_report_path }} <<EOL
    #     ***********************************************************
    #     ************** Testcase Functionality Report **************
    #     ***********************************************************

    #     EOL

    - name: Handle Skip Requests
      run: |
        python .github/workflows/skip_handler.py --actor "${{ github.actor }}" --title '${{ github.event.pull_request.title }}' --check func | tee -a ${{ env.testcase_func_report_path }}

    - name: Pre-job-submission prep
      if: ${{env.SKIP_CHECK == 'false' && (env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true')}}
      run: |
        # TODO: get this function working
        ################################
        # run_curl () {
        #   echo $1
        #   echo "$1"
        #   curl_output=$( curl $1 )
        #   curl_return_code=$?

        #   if (( $curl_return_code != 0 )); then
        #     echo "Error contacting to AutoIQ. Message: ${curl_output}"
        #     exit 1
        #   fi
        # }

        # # Get a session token for this run
        # curl_command="-X GET --no-progress-meter \"http://autoiq.extremenetworks.com/auth/getSessionTokenFromPAT\" -H \"accept: application/json\" -H \"authorization: PAT ${{ secrets.AUTOIQ_PAT }}\""
        # echo $curl_command
        # run_curl "$curl_command"
        # session_token=$( echo "$curl_command" | jq -r '.result.sessionToken' )
        # echo $curl_output
        ################################

        ### Get Session token
        curl_return_code=0
        curl_output=$( curl -X GET  --no-progress-meter \
                        "http://autoiq.extremenetworks.com/auth/getSessionTokenFromPAT" \
                        -H "accept: application/json" \
                        -H "authorization: PAT ${{ secrets.AUTOIQ_PAT }}" )
        curl_return_code=$?

        if (( $curl_return_code != 0 )); then
          echo "Error getting session token. Message: ${curl_output}"
          exit 1
        else
          session_token=$( echo "${curl_output}" |  jq -r '.result.sessionToken' )
        fi

        ### Get list of available testbeds for each harness
        # harnesses: pytest, robot, tcl
        # 11111111-2222-2222-2222-111111111111 (pytest), 11111111-6666-6666-6666-111111111111 (robot), 11111111-5555-5555-5555-111111111111 (tcl)
        curl_return_code=0
        curl_output=$( curl -X GET --no-progress-meter \
                                    "http://autoiq.extremenetworks.com/tbedmgr/jobmgr/cicdPlatforms/pytest" \
                                    -H "accept: application/json" \
                                    -H "authorization: Bearer ${session_token}" )
        curl_return_code=$?

        if (( $curl_return_code != 0 )); then
          echo "Error getting session token. Message: ${curl_output}"
          exit 1
        else
          available_testbeds_pytest=$( echo "${curl_output}" | jq '.result[]' )
        fi

        curl_return_code=0
        curl_output=$( curl -X GET --no-progress-meter \
                                    "http://autoiq.extremenetworks.com/tbedmgr/jobmgr/cicdPlatforms/robot" \
                                    -H "accept: application/json" \
                                    -H "authorization: Bearer ${session_token}" )
        curl_return_code=$?

        if (( $curl_return_code != 0 )); then
          echo "Error getting session token. Message: ${curl_output}"
          exit 1
        else
          available_testbeds_robot=$( echo "${curl_output}" | jq '.result[]' )
        fi

        echo "pytest testbeds: $available_testbeds_pytest"
        echo "robot testbeds: $available_testbeds_robot"

        # echo "ROBOT_TESTBEDS=${available_testbeds_robot}" >> $GITHUB_ENV
        # echo "PYTEST_TESTBEDS=${available_testbeds_pytest}" >> $GITHUB_ENV
        echo "SESSION_TOKEN=${session_token}" >> $GITHUB_ENV


    # Submit job for each platform on each NOS that runs all the tests
    #
    - name: Run Modified TestCases
      if: ${{env.SKIP_CHECK == 'false' && (env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true')}}
      run: |
        # CreateJob body template
        job_template='{
                "jsonString":{
                    "description": "CI Verification Run",
                    "priority": 3,
                    "version": 1,
                    "rerun": 0,
                    "postResults": 1,
                    "notifyOnStart": 0,
                    "username": "CI_Service",
                    "group": "CICD",
                    "jobType": "private",
                    "auxInfoList" : [
                      {"cicd_pr":1},
                      {"context": "${{ github.event.repository.name }}/test-context"},
                      {"repo":"${{ github.repository }}"},
                      {"sha":"${{ github.sha }}"},
                      {"state":"pending"}
                    ],
                    "jobPlatforms": [
                      {
                        "platform": "EXOS-DT",
                        "hardwareConfiguration": "Standalone",
                        "priority": 3,
                        "auxInfoList": [],
                        "jobPlatformTestModules" :
                        [
                            {
                                "testModule_uuid": $harness_uuid,
                                "auxInfoList" : [
                                                  {"nodeCount": 4},
                                                  {"universalTestBedOs": "EXOS"},
                                                  {"test_name": $tests }
                                                  ]
                            },
                            {
                                "testModule_uuid": $harness_uuid,
                                "auxInfoList" : [
                                                  {"nodeCount": 4},
                                                  {"universalTestBedOs": "VOSS"},
                                                  {"test_name": $tests }
                                                  ]
                            }
                        ]
                      }
                    ]
                }
              }'

        # Format Test Lists and add "extreme_automation_tests" prefix to each path
        pytest_tests=$( cat ${pytest_func_path} | jq -ncR 'inputs | split(",") | [ .[] | "extreme_automation_tests/" + . ]' )
        robot_tests=$( cat ${robot_func_path} | jq -ncR 'inputs | split(",") | [ .[] | "extreme_automation_tests/" + . ]' )

        pytest_harness_uuid="11111111-2222-2222-2222-111111111111"
        robot_harness_uuid="11111111-6666-6666-6666-111111111111"

        printf 'Pytest tests to run: %s\nRobot tests to run: %s\n' "$pytest_tests" "$robot_tests"

    # Commented out on 5/19 by petersadej
    # Will be uncommented when the system to set PR check status based on the outcome of the test is reworked.
    # if [[ "${{ env.RUN_ROBOT }}" == "true" ]]; then

    #     # Generate robot JSON body
    #     CURL_BODY=$(jq --null-input \
    #               --argjson tests "$robot_tests" \
    #               --arg harness_uuid "$robot_harness_uuid" \
    #               "$job_template")

    #     printf '%s\n\n' "[*] Robot Job Submit JSON: $CURL_BODY" | tee -a ${{ env.testcase_func_report_path }}

    #     curl -X POST --no-progress-meter \
    #     "https://autoiq.extremenetworks.com/tbedmgr/jobmgr/createJob" \
    #     -H "accept: application/json" \
    #     -H "Content-Type: application/json" \
    #     -H "authorization: Bearer ${SESSION_TOKEN}" \
    #     -d "${CURL_BODY}"
    # fi

    # if [[ "${{ env.RUN_PYTEST }}" == "true" ]]; then

    #     # Generate pytest JSON body
    #     CURL_BODY=$(jq --null-input \
    #               --argjson tests "$pytest_tests" \
    #               --arg harness_uuid "$pytest_harness_uuid" \
    #               "$job_template")

    #     printf '%s\n\n' "[*] Pytest Job Submit JSON: $CURL_BODY" | tee -a ${{ env.testcase_func_report_path }}

    #     curl -X POST --no-progress-meter \
    #     "https://autoiq.extremenetworks.com/tbedmgr/jobmgr/createJob" \
    #     -H "accept: application/json" \
    #     -H "Content-Type: application/json" \
    #     -H "authorization: Bearer ${SESSION_TOKEN}" \
    #     -d "${CURL_BODY}"
    # fi


    # Commented out on 4/21/22 by petersadej.
    # This report is currently broken due to changes outside of this script. All or parts of this can be used in the future
    #
    # - name: Upload test results to GH artifacts
    #   uses: actions/upload-artifact@v2
    #   if: ${{ always() }}
    #   with:
    #     name: ${{ env.report_artifact_name }}
    #     path: |
    #       ${{ env.testcase_func_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "::set-output name=status::${{ job.status }}"

  handle_results:
    needs: [check_tests_functionality, check_file, check_names, check_tags, check_testbeds, check_dir]
    name: Send result emails
    if: ${{ always() }}
    runs-on: ubuntu-latest
    env:
      EMAIL_BODY_FILE: body.html

    steps:
    - name: Checkout
      uses: actions/checkout@v2

    - uses: actions/download-artifact@v2
      with:
        name: ${{ env.report_artifact_name }}

    - name: Set environment variables
      run: |
        echo 'github actor: ${{ github.actor }}'
        actor_email=$( jq -r '."${{ github.actor }}"' .github/workflows/github_names_to_extr_email.json )
        echo "ACTOR_EMAIL=${actor_email}" >> $GITHUB_ENV

        # Set default status values for email subject line and body
        cat > ${EMAIL_BODY_FILE} << EOL
        <h3>Results:</h3>

        <p>Testbed file validation:            ${{ needs.check_testbeds.outputs.status }}</p>

        <p>File name and location validation:  ${{ needs.check_file.outputs.status }}</p>

        <p>Directory structure validation:     ${{ needs.check_dir.outputs.status }}</p>

        <p>Testcase naming validation:         ${{ needs.check_names.outputs.status }}</p>

        <p>Tags/Markers validation:            ${{ needs.check_tags.outputs.status }}</p>

        <p>Test functionality validation:      ${{ needs.check_tests_functionality.outputs.status }}</p>
        <hr>

        <p>The log files from your run of ${{ github.workflow }} are attached to this email.</p>

        <p>The full results for this run can be seen here: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}.</p>
        EOL

        # Set overall workflow status
        if [[ "${{ needs.check_testbeds.outputs.status }}" == "failure" || "${{ needs.check_file.outputs.status }}" == "failure" || "${{ needs.check_dir.outputs.status }}" == "failure" || "${{ needs.check_names.outputs.status }}" == "failure" || "${{ needs.check_tags.outputs.status }}" == "failure" || "${{ needs.check_tests_functionality.outputs.status }}" == "failure" ]]; then
          echo "WORKFLOW_STATUS=Failed" >> $GITHUB_ENV
        else
          echo "WORKFLOW_STATUS=Passed" >> $GITHUB_ENV
        fi

    - name: Email results
      uses: dawidd6/action-send-mail@v3
      continue-on-error: true
      with:
        # Required mail server address:
        server_address: smtp.office365.com
        # Required mail server port:
        server_port: 587
        # Mail server username:
        username: srv-econ-notify@extremenetworks.com
        # Mail server password:
        password: "[kkwm{Y-36{k"
        # Required mail subject:
        subject: CI Test ${{ env.WORKFLOW_STATUS }} - ${{ github.repository }}
        # Required recipients' addresses:
        to: ${{ env.ACTOR_EMAIL }}
        # Required sender full name (address can be skipped):
        from: srv-econ-notify@extremenetworks.com
        # Optional whether this connection use TLS (default is true if server_port is 465)
        secure: false
        # Optional plain body:
        #body: ${{ env.EMAIL_BODY }}
        # Optional HTML body read from file:
        html_body: file://${{ env.EMAIL_BODY_FILE }}
        # Optional unsigned/invalid certificates allowance:
        ignore_cert: true
        # Optional converting Markdown to HTML (set content_type to text/html too):
        convert_markdown: true
        # Attachments: all text report files in current dir
        attachments: ./*_report.txt
