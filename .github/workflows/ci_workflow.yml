name: CI Workflow
# Ensures only one instance of the CI workflow is running per branch
# This stops multiple rapid commits from holding up the process
concurrency: ci-${{ github.ref }}

on:
  workflow_dispatch:

  pull_request:
    branches: [main, RELEASE-**, 2022_Q1r2]

env:
  # List of users authorized to skip checks and perform other admin operations
  authorized_users: |
    petersadej
    ghundertmark
    elatour2000
    jeffrwsd
    jsubler93
  pytest_func_path:             pytest_func_tests.txt
  pytest_non_prod_path:         pytest_non_prod_tests.txt
  pytest_libs_path:             pytest_libs.txt
  pytest_res:                   pytest_resources.txt
  robot_func_path:              robot_func_tests.txt
  robot_non_prod_path:          robot_non_prod_tests.txt
  robot_libs_path:              robot_libs.txt
  testbed_files_path:           testbeds.txt
  other_files_path:             other_files.txt
  changed_files_artifact_name:  changed-files
  testbed_files_report_path:    testbed_files_report.txt
  testcase_files_report_path:   testcase_files_report.txt
  directory_struct_report_path: directory_structure_report.txt
  testcase_naming_report_path:  testcase_naming_report.txt
  tag_marker_report_path:       tag_and_marker_report.txt
  testcase_func_report_path:    testcase_functionality_report.txt
  report_artifact_name:         CI-reports-${{ github.event.repository.name }}-run-${{ github.run_number }}
  documentation_url:            https://extremenetworks2com.sharepoint.com/:w:/r/sites/qa-extauto/_layouts/15/Doc.aspx?sourcedoc=%7B357F1F2C-B274-4739-9D87-A67A72875846%7D&file=Automation%20Process%20-%20Draft.docx

jobs:
  check_changes:
    name: Check for changes in test scripts
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v2

    # https://github.com/tj-actions/changed-files
    - name: Get changed pytest functional tests
      id: pytest-func
      uses: tj-actions/changed-files@v10
      with:
        files: |
          Tests/Pytest/Functional/([^/]+/)+TestCases/
        separator: ','

    - name: Get changed pytest non-prod tests
      id: pytest-non-prod
      uses: tj-actions/changed-files@v10
      with:
        files: |
          Tests/Pytest/NonProduction/([^/]+/)+TestCases/
        separator: ','

    - name: Get changed pytest resources
      id: pytest-resources
      uses: tj-actions/changed-files@v10
      with:
        files: |
          Tests/Pytest/([^/]+/)+Resources/
        separator: ','

    - name: Get changed robot functional tests
      id: robot-func
      uses: tj-actions/changed-files@v10
      with:
        files: |
          Tests/Robot/Functional/([^/]+/)+TestCases/
        separator: ','

    - name: Get changed robot non-prod tests
      id: robot-non-prod
      uses: tj-actions/changed-files@v10
      with:
        files: |
          Tests/Robot/NonProduction/([^/]+/)+TestCases/
        separator: ','

    - name: Get changed robot libraries
      id: robot-libs
      uses: tj-actions/changed-files@v10
      with:
        files: |
          Tests/Robot/Libraries/([^/]+/)+
        separator: ','

    - name: Get changed pytest libraries
      id: pytest-libs
      uses: tj-actions/changed-files@v10
      with:
        files: |
          Tests/Pytest/Libraries/([^/]+/)+
        separator: ','

    - name: Get changed Testbed YAML files
      id: testbed
      uses: tj-actions/changed-files@v10
      with:
        files: |
          TestBeds
        separator: ','

    # Outputs a list of files breaking directory rules that we do and don't care about
    - name: Get other changed files
      id: other-files
      uses: tj-actions/changed-files@v10
      with:
        files: |
          Tests/Pytest/(Functional|NonProduction)/([^/]+/)+(TestCases|Resources)/
          Tests/Robot/(Functional|NonProduction)/([^/]+/)+(TestCases|Resources)/
          Tests/(Pytest|Robot)/Libraries/([^/]+/)+
          Tests/(Pytest|Robot)/Demos/
          __init__.py
        separator: ','

    # Outputs a list of files breaking directory rules that we don't care about
    - name: Get other changed files we dont care about
      id: other-files2
      uses: tj-actions/changed-files@v10
      with:
        files: |
          Tests
        separator: ','

    # Diff two lists to get just the files we care about
    # All files in this list need to conform to directory structure rules
    - name: Diff other files lists
      shell: python
      run: |
        l1 = "${{ steps.other-files.outputs.other_changed_files }}".split(",")
        l2 = "${{ steps.other-files2.outputs.other_changed_files }}".split(",")
        diff_list = [x for x in l1 if x not in l2]
        diff_string = ",".join(diff_list)
        print(diff_string)
        with open('${{ env.other_files_path }}', 'w') as outfile:
          outfile.write(diff_string)


    - name: List all modified files
      env:
        PY_FUNC:         ${{ steps.pytest-func.outputs.all_modified_files }}
        PY_NON_PROD:     ${{ steps.pytest-non-prod.outputs.all_modified_files }}
        PY_LIBS:         ${{ steps.pytest-libs.outputs.all_modified_files }}
        PY_RES:          ${{ steps.pytest-resources.outputs.all_modified_files }}
        ROBOT_FUNC:      ${{ steps.robot-func.outputs.all_modified_files }}
        ROBOT_NON_PROD:  ${{ steps.robot-non-prod.outputs.all_modified_files }}
        ROBOT_LIBS:      ${{ steps.robot-libs.outputs.all_modified_files }}
        TESTBED_FILES:   ${{ steps.testbed.outputs.all_modified_files }}
        OTHER_FILES:     ${{ steps.other-files.outputs.other_changed_files }}
      run: |
        echo $PY_FUNC > ${{ env.pytest_func_path }}
        echo $PY_NON_PROD > ${{ env.pytest_non_prod_path }}
        echo $PY_LIBS > ${{ env.pytest_libs_path }}
        echo $PY_RES > ${{ env.pytest_res }}
        echo $ROBOT_FUNC > ${{ env.robot_func_path }}
        echo $ROBOT_NON_PROD > ${{ env.robot_non_prod_path }}
        echo $ROBOT_LIBS > ${{ env.robot_libs_path }}
        echo $TESTBED_FILES > ${{ env.testbed_files_path }}
        # echo $OTHER_FILES > ${{ env.other_files_path }}

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v2
      with:
        name: ${{ env.changed_files_artifact_name }}
        path: |
          ${{ env.pytest_func_path }}
          ${{ env.pytest_non_prod_path }}
          ${{ env.pytest_libs_path }}
          ${{ env.pytest_res }}
          ${{ env.robot_func_path }}
          ${{ env.robot_non_prod_path }}
          ${{ env.robot_libs_path }}
          ${{ env.testbed_files_path }}
          ${{ env.other_files_path }}

  check_testbeds:
    needs: [check_changes]
    name: Testbed file validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}

    steps:
    - name: Checkout
      uses: actions/checkout@v2

    - uses: actions/download-artifact@v2
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        # Create report file
        cat >${{ env.testbed_files_report_path }} <<EOL
        ******************************************************
        ************** Testbed YAML File Report **************
        ******************************************************

        EOL

    - name: Check for lowercase keys in YAML
      run: |
        python .github/workflows/testbed_yaml_parser.py ${{ env.testbed_files_path }} | tee -a ${{ env.testbed_files_report_path }}

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v2
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.testbed_files_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "::set-output name=status::${{ job.status }}"

  check_file:
    needs: [check_changes]
    name: File name and location validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}

    steps:
    - uses: actions/download-artifact@v2
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        # Create report file
        cat >${{ env.testcase_files_report_path }} <<EOL
        *****************************************************
        ********** Robot/PyTest file Naming Report **********
        *****************************************************

        EOL

    - name: Process Pytest Production file(s)
      run: |
        IFS=','
        rc=0
        files_tested=0
        for f in $( cat ${{ env.pytest_func_path }} | sed -e 's/ //g' ); do
          let "files_tested += 1"
          printf "[*] Processing Pytest file: $f ...\n" | tee -a ${{ env.testcase_files_report_path }}
          if [[ ! $f =~ .*\.py$ ]] ; then
              printf "[*] FAIL: $f is NOT a Pytest file!  Only .py files are allowed in this directory.\n" | tee -a ${{ env.testcase_files_report_path }}
              rc=1
          # Only allow __init__.py or files that conform to "test_<testname>_<valid_jira_id>.py"
          # valid jira id: One or more letters followed by one or more numbers. A dash between the letters and numbers is optional.
          # This can potentially be followed by _ and 1 or more numbers if multiple testcase files are needed for the same id.
          # Examples: test_blah_app-45   test_blah_blah_app45_1   test_blah_app-45_1   test_blah_app45
          # Note: inside bash [[ ]] you need to use extended regex format
          elif [[ ! $f =~ test_.*_[a-zA-Z]+-?[0-9]+(_[0-9]+\.py|\.py)$|__init__\.py$ ]] ; then
              echo "[*] FAIL: $f is NOT in a valid Pytest filename format!" | tee -a ${{ env.testcase_files_report_path }}
              echo '[*] For more information see the Test Suite File Name section of ${{ env.documentation_url }}' | tee -a ${{ env.testcase_files_report_path }}
              rc=1
          else
              printf "[*] PASS: $f\n" | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          echo "[*] No Pytest Production files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Process Pytest Non-Production file(s)
      if: ${{ always() }}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for f in $( cat ${{ env.pytest_non_prod_path }} | sed -e 's/ //g' ); do
          let "files_tested += 1"
          printf "[*] Processing Pytest file: $f ...\n" | tee -a ${{ env.testcase_files_report_path }}
          if [[ ! $f =~ .*\.py$ ]] ; then
              printf "[*] FAIL: $f is NOT a Pytest file! Only .py files are allowed in this directory.\n" | tee -a ${{ env.testcase_files_report_path }}
              rc=1
          # Only allow __init__.py or files that conform to "test_<testname>.py"
          # Note: inside bash [[ ]] you need to use extended regex format
          elif [[ ! "$f" =~ test_[^/\\]*\.py$|__init__\.py$ ]] ; then
              echo "[*] FAIL: $f is NOT in a valid Pytest filename format!" | tee -a ${{ env.testcase_files_report_path }}
              echo '[*] For more information see the Test Suite File Name section of ${{ env.documentation_url }}' | tee -a ${{ env.testcase_files_report_path }}
              rc=1
          else
              printf "[*] PASS: $f\n" | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          echo "[*] No Pytest Non-Production files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Process Pytest Library file(s)
      if: ${{ always() }}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for f in $( cat ${{ env.pytest_libs_path }} | sed -e 's/ //g' ); do
          let "files_tested += 1"
          printf "[*] Processing Python file: $f ...\n" | tee -a ${{ env.testcase_files_report_path }}
          if [[ ! $f =~ .*\.py$ ]] ; then
              printf "FAIL: $f is NOT a Python file! Only .py files are allowed in this directory.\n" | tee -a ${{ env.testcase_files_report_path }}
              rc=1
          else
              printf "PASS: $f\n" | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          echo "[*] No Pytest Library files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Process Robot Production file(s)
      if: ${{ always() }}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for f in $( cat ${{ env.robot_func_path }} | sed -e 's/ //g' ); do
          let "files_tested += 1"
          printf "[*] Processing Robot file: $f ...\n" | tee -a ${{ env.testcase_files_report_path }}
          if [[ ! $f =~ .*\.robot$ ]] ; then
              printf "[*] FAIL: $f is NOT a Robot file! Only .robot files are allowed in this directory.\n" | tee -a ${{ env.testcase_files_report_path }}
              rc=1
          # Only allow files that conform to "<testname>_<valid_jira_id>.robot" or "__init__.robot"
          # See "Process Pytest Production file(s)" for definition of <valid_jira_id>
          elif [[ ! $f =~ _[a-zA-Z]+-?[0-9]+(_[0-9]+\.robot|\.robot)$|__init__\.robot$ ]] ; then
              echo "[*] FAIL: $f is NOT in a valid Robot filename format!" | tee -a ${{ env.testcase_files_report_path }}
              echo '[*] For more information see the Test Suite File Name section of ${{ env.documentation_url }}' | tee -a ${{ env.testcase_files_report_path }}
              rc=1
          else
              printf "[*] PASS: $f\n"
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          echo "[*] No Robot Production files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Process Robot Non-Production file(s)
      if: ${{ always() }}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for f in $( cat ${{ env.robot_non_prod_path }} | sed -e 's/ //g' ); do
          let "files_tested += 1"
          printf "[*] Processing Robot file: $f ...\n" | tee -a ${{ env.testcase_files_report_path }}
          if [[ ! $f =~ .*\.robot$ ]] ; then
              printf "FAIL: $f is NOT a Robot file! Only .robot files are allowed in this directory.\n" | tee -a ${{ env.testcase_files_report_path }}
              rc=1
          else
              printf "PASS: $f\n" | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          echo "[*] No Robot Non-Production files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Process Robot Library file(s)
      if: ${{ always() }}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for f in $( cat ${{ env.robot_libs_path }} | sed -e 's/ //g' ); do
          let "files_tested += 1"
          printf "[*] Processing Robot file: $f ...\n" | tee -a ${{ env.testcase_files_report_path }}
          if [[ ! $f =~ .*\.robot$ ]] ; then
              printf "FAIL: $f is NOT a Robot file! Only .robot files are allowed in this directory.\n" | tee -a ${{ env.testcase_files_report_path }}
              rc=1
          else
              printf "PASS: $f\n" | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          echo "[*] No Robot Library files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v2
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.testcase_files_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "::set-output name=status::${{ job.status }}"

  check_dir:
    needs: [check_changes]
    name: Directory structure validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}

    steps:
    - uses: actions/download-artifact@v2
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        # Create report file
        cat >${{ env.directory_struct_report_path }} <<EOL
        ********************************************************
        ************** Directory Structure Report **************
        ********************************************************

        EOL

    - name: Check if any disallowed directories were created
      run: |
        trimmed_files=$(cat ${{ env.other_files_path }} | tr -d [:space:])
        if [[ $trimmed_files ]]; then
          echo "[*] FAIL: New directories and files not allowed in protected areas." | tee -a ${{ env.directory_struct_report_path }}
          echo '[*] For more information see the Test Suite Location section of ${{ env.documentation_url }}' | tee -a ${{ env.directory_struct_report_path }}
          echo "[*] Offending files/directories:" | tee -a ${{ env.directory_struct_report_path }}

          IFS=','
          for path in $trimmed_files ; do
            echo "[**] $path" | tee -a ${{ env.directory_struct_report_path }}
          done
          exit 1
        else
          echo "[*] PASS: No files/directories found in protected areas." | tee -a ${{ env.directory_struct_report_path }}
        fi

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v2
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.directory_struct_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "::set-output name=status::${{ job.status }}"

  check_names:
    needs: [check_file, check_dir]
    name: Testcase naming validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}

    steps:
    - name: Checkout modified files
      uses: actions/checkout@v2

    - uses: actions/download-artifact@v2
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        cat >${{ env.testcase_naming_report_path }} <<EOL
        ******************************************************
        *************** Testcase Naming Report ***************
        ******************************************************

        EOL

    # TODO: fix this for __init__.py files
    - name: Process modified Pytest Resources files
      run: |
        files_tested=0
        for line in $( cat ${{ env.pytest_res }} | tr ',' '\n' ) ; do
          let "files_tested += 1"
          printf "[*] Processing Pytest Resources file: $line ...\n" | tee -a ${{ env.testcase_naming_report_path }}
            for func in `grep -w def $line | awk '{print $2}'`; do
              printf "Testing Function ${func} ...\n" | tee -a ${{ env.testcase_naming_report_path }}
              if [[ ${func} =~ ^test_.*$ ]] ; then
                printf "Function ${func}: FAIL\n" | tee -a ${{ env.testcase_naming_report_path }}
                exit 1
              else
                printf "Function ${func}: PASS\n" | tee -a ${{ env.testcase_naming_report_path }}
              fi
            done
        done
        if (( files_tested == 0 )); then
          echo "[*] No Pytest Resource files found. Skipping these tests..." | tee -a ${{ env.testcase_naming_report_path }}
        fi

    - name: Process modified Pytest Test files
      run: |
        files_tested=0
        for line in $( cat ${{ env.pytest_func_path }} ${{ env.pytest_non_prod_path }} | tr ',' '\n' ) ; do
          let "files_tested += 1"
          printf "[*] Processing Pytest Test file: $line ...\n" | tee -a ${{ env.testcase_naming_report_path }}
            func=`grep -w def $line | awk '{print $2}' | grep ^test_`
              if [[ ${line} =~ __init__\.py ]] ; then
                printf "File ${line}: PASS\n" | tee -a ${{ env.testcase_naming_report_path }}
              elif [[ ${func-} ]] ; then
                printf "File ${line}: PASS\n" | tee -a ${{ env.testcase_naming_report_path }}
              else
                printf "File ${line}: FAIL\n" | tee -a ${{ env.testcase_naming_report_path }}
                exit 1
              fi
        done
        if (( files_tested == 0 )); then
          echo "[*] No Pytest Test files found. Skipping these tests..." | tee -a ${{ env.testcase_naming_report_path }}
        fi

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v2
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.testcase_naming_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "::set-output name=status::${{ job.status }}"

  check_tags:
    needs: [check_names]
    name: Tags/Markers validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}

    steps:
    - uses: actions/download-artifact@v2
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        # Create report file
        cat >${{ env.tag_marker_report_path }} <<EOL
        ********************************************************
        ************** Testcase Tag/Marker Report **************
        ********************************************************

        EOL

        # Check if any prod pytest files have been modified
        # pytest_files=$(cat ${{ env.pytest_func_path }} ${{ env.pytest_non_prod_path }} | tr ',\n' ' ')
        pytest_files=$(cat ${{ env.pytest_func_path }} | tr ',\n' ' ')
        trimmed_files=$(echo "${pytest_files}" | tr -d [:space:])
        if [[ ! $trimmed_files ]]; then
          run_pytest=false
          echo "[*] No PyTest Test files found. Skipping these tests..." | tee -a ${{ env.tag_marker_report_path }}
        else
          run_pytest=true
        fi
        echo "Run pyTest: $run_pytest"

        # Check if any prod robot files have been modified
        # robot_files=$(cat ${{ env.robot_func_path }} ${{ env.robot_non_prod_path }} | tr ',\n' ' ')
        robot_files=$(cat ${{ env.robot_func_path }} | tr ',\n' ' ')
        trimmed_files=$(echo "${robot_files}" | tr -d [:space:])
        if [[ ! $trimmed_files ]]; then
          run_robot=false
          echo "[*] No Robot Test files found. Skipping these tests..." | tee -a ${{ env.tag_marker_report_path }}
        else
          run_robot=true
        fi
        echo "Run Robot: $run_robot"

        # Set env vars
        echo "RUN_PYTEST=${run_pytest}" >> $GITHUB_ENV
        echo "RUN_ROBOT=${run_robot}" >> $GITHUB_ENV
        echo "PYTEST_FILES=${pytest_files}" >> $GITHUB_ENV
        echo "ROBOT_FILES=${robot_files}" >> $GITHUB_ENV

    # Only do the below steps if we need to evaluate any of the changes being committed
    - name: Checkout local repo
      if: ${{env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true'}}
      uses: actions/checkout@v2

    - name: Handle Skip Requests
      if: ${{env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true'}}
      run: |
        # Check for CI skip flag in PR title
        skip_requested=${{ startsWith(github.event.pull_request.title, '[Skip Checks: Tags]') }}
        skip_check=false
        current_user=${{ github.actor }}
        if [[ $skip_requested == 'true' ]]; then
            # Verify the user is allowed to skip checks
            # TODO: Come back this. I tried putting this list in a separate file, but there is some sort of bug in GitHub actions preventing me from reading the file here. See: https://github.com/extremenetworks/extreme_automation_tests/runs/5724285683?check_suite_focus=true
            for user in ${authorized_users}; do
                if [[ $user == $current_user ]]; then
                    skip_check=true
                fi
            done
            if [[ $skip_check == 'true' ]]; then
                echo "[*] Tests skipped by an authorized user..." | tee -a ${{ env.tag_marker_report_path }}
            else
                echo "[*] Tests skip requested, but current user is not authorized. Exiting..." | tee -a ${{ env.tag_marker_report_path }}
                exit 1
            fi
        fi

        # Set env var
        echo "SKIP_CHECK=${skip_check}" >> $GITHUB_ENV

    - name: Checkout framework
      if: ${{env.SKIP_CHECK == 'false' && (env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true')}}
      uses: actions/checkout@v2
      with:
        repository: extremenetworks/extreme_automation_framework
        token: ${{ secrets.REGISTRY_PAT }}
        path: extreme_automation_framework
        ref: main

    - name: Checkout qtest tools
      if: ${{env.SKIP_CHECK == 'false' && (env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true')}}
      uses: actions/checkout@v2
      with:
        repository: extremenetworks/econ-qtest-client
        token: ${{ secrets.REGISTRY_PAT }}
        path: econ-qtest-client
        ref: main

    - uses: actions/download-artifact@v2
      if: ${{env.SKIP_CHECK == 'false' && (env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true')}}
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Install python requirements
      if: ${{env.SKIP_CHECK == 'false' && (env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true')}}
      run: |
        grep -v pycurl requirements.txt > requirements.txt.tmp && mv requirements.txt.tmp requirements.txt
        pip install -r requirements.txt

    # TODO: Figure out how to get pytest to only operate on input files. If that isn't viable
    #       run for all files and only grab the ones we want(would take SIGNIFICANTLY more time)
    # potential workaround: copy modified tests to another directory and run pytest there
    # this would require either making a whole mimiced dir tree or using a different method of identifying the tests
    - name: Run pytest inventory tool
      if: ${{env.SKIP_CHECK == 'false' && env.RUN_PYTEST == 'true'}}
      run: |
        # Add framework to PYTHONPATH
        export PYTHONPATH="${PYTHONPATH}:${PWD}/extreme_automation_framework"

        # Run pytest
        pytest --get_test_info cicd ${{ env.PYTEST_FILES }} || true

    - name: Run robot inventory tool
      if: ${{env.SKIP_CHECK == 'false' && env.RUN_ROBOT == 'true'}}
      env:
        ROBOT_TOOL_PATH: Tests/Robot/get_test_info.py
      run: |
        # Add framework to PYTHONPATH
        export PYTHONPATH="${PYTHONPATH}:${PWD}/extreme_automation_framework"

        # Run robot
        python $ROBOT_TOOL_PATH ${{ env.ROBOT_FILES }}

    # Tags and Markers:
    # Tests should contain Tags (Robot) or Markers (pyTest) to categorize the tests allowing test case selection during a test run.  This allows the automation system to select or omit specific tests.  For example, if tests are specific to a particular device and cannot be run on other devices you would want to add a tag or marker to identify the hardware the test can run on.   The number and types of tags are essentially limitless.  To avoid clutter please review existing tags and reuse them where possible.
    #
    # The following tags will be selected by the automation team, test developers must avoid using them (CI/CD will check for this):
    # production, regression, nightly, sanity, p1, p2, p3, p4
    #
    # All tags or markers should be in lower case only.
    # Developers must have two tags or markers.  The first tag or marker will identify the qTest project and test case number in the format: <project>_tc_<test case number>.  The second tag or marker must be: “development”, indicating that this test case is new to the automation system.
    # The project names are short names that identify the qTest project.  The names and their matching project are listed below.
    #
    # Project Names
    # csit -> csit
    # xim -> XIQ Mainline
    # exos -> EXOS
    # voss -> VOSS
    # xiq -> xiq
    # xnt -> EXOS-NOS-TestPlans
    # xna -> EXOS-NOS-UnifiedAgile
    #
    #  Robot:
    # [Tags] csit_tc_4501    development
    #
    # PyTest:
    # @pytest.mark.csit_tc_4501
    # @pytest.mark.development
    #
    # Identifying the TestBed
    # The type of testbed must be specified in the test suite.  This should be done by specifying the testbed type using “Force Tags” in robot or by marking the class in PyTest.  Possible options for the testbed type are limited to:
    #
    # testbed_1_node
    # testbed_2_node
    # testbed_3_node
    # testbed_4_node
    # testbed_5_node
    #
    # Robot:
    # Robot uses “Force Tags” to mark all of the tests with the same tag.  The TestBed type must be identified in each Test Case.  Using “Force Tags” you can mark every test.  Here is an example:
    #
    # *** Settings ***
    # Force Tags      testbed_1_node
    #
    # PyTest:
    # To mark every test with the proper testbed you can mark the entire class with the testbed.  Here is an example:
    #
    # @mark.testbed_1_node
    # Class DefaultTests:
    #
    - name: Check pytest markers
      if: ${{env.SKIP_CHECK == 'false' && env.RUN_PYTEST == 'true'}}
      run: |
        rc=0
        testcase_files=$( jq -r 'to_entries | map(.key + "|" + (.value | tostring)) | .[]' pytest_data.json )
        while IFS='|' read key value; do
            echo "================================================================================" | tee -a ${{ env.tag_marker_report_path }}
            echo "[*] Results for file: $key" | tee -a ${{ env.tag_marker_report_path }}
            func=$( jq -r 'to_entries | map(.key + "|" + (.value | tostring)) | .[]' <<< $value )
            while IFS='|' read key value; do
                echo "[**] Results for test: $key" | tee -a ${{ env.tag_marker_report_path }}
                # All lowercase check
                result=$(jq -r '.all_tags_lower_case' <<< $value )
                if [[ $result == 'false' ]]; then
                    echo "[***] FAIL: One or more markers contain uppercase characters" | tee -a ${{ env.tag_marker_report_path }}
                    rc=1
                fi
                # doesn't contain reserved tag check
                result=$(jq -r '.contains_reserved_tag' <<< $value)
                if [[ $result == 'true' ]]; then
                    echo "[***] FAIL: One or more reserved markers were found. Reserved markers: [production, regression, nightly, sanity, p1, p2, p3, p4]" | tee -a ${{ env.tag_marker_report_path }}
                    rc=1
                fi
                # contains development tag check
                result=$(jq -r '.contains_development' <<< $value )
                if [[ $result == 'false' ]]; then
                    echo "[***] FAIL: 'development' marker not found" | tee -a ${{ env.tag_marker_report_path }}
                    rc=1
                fi
                # contains testbed tag check
                result=$(jq -r '.contains_testbed_tag' <<< $value )
                if [[ $result == 'false' ]]; then
                    echo "[***] FAIL: Testbed type marker not found" | tee -a ${{ env.tag_marker_report_path }}
                    rc=1
                fi
                # contains qtest tag check
                result=$(jq -r '.valid_qtest_tag' <<< $value )
                if [[ $result == 'false' ]]; then
                    echo "[***] FAIL: qtest marker invalid or not found" | tee -a ${{ env.tag_marker_report_path }}
                    rc=1
                fi
            done <<< $func
            echo "" | tee -a ${{ env.tag_marker_report_path }}
        done <<< $testcase_files
        exit $rc

    - name: Check robot tags
      if: ${{env.SKIP_CHECK == 'false' && env.RUN_ROBOT == 'true'}}
      run: |
        rc=0
        testcase_files=$( jq -r 'to_entries | map(.key + "|" + (.value | tostring)) | .[]' robot_data.json )
        while IFS='|' read key value; do
            echo "================================================================================" | tee -a ${{ env.tag_marker_report_path }}
            echo "[*] Results for file: $key" | tee -a ${{ env.tag_marker_report_path }}
            func=$( jq -r 'to_entries | map(.key + "|" + (.value | tostring)) | .[]' <<< $value )
            while IFS='|' read key value; do
                echo "[**] Results for test: $key"
                # All lowercase check
                result=$(jq -r '.all_tags_lower_case' <<< $value )
                if [[ $result == 'false' ]]; then
                    echo "[***] FAIL: One or more markers contain uppercase characters" | tee -a ${{ env.tag_marker_report_path }}
                    rc=1
                fi
                # doesn't contain reserved tag check
                result=$(jq -r '.contains_reserved_tag' <<< $value)
                if [[ $result == 'true' ]]; then
                    echo "[***] FAIL: One or more reserved markers were found. Reserved markers: [production, regression, nightly, sanity, p1, p2, p3, p4]" | tee -a ${{ env.tag_marker_report_path }}
                    rc=1
                fi
                # contains development tag check
                result=$(jq -r '.contains_development' <<< $value )
                if [[ $result == 'false' ]]; then
                    echo "[***] FAIL: 'development' marker not found" | tee -a ${{ env.tag_marker_report_path }}
                    rc=1
                fi
                # contains testbed tag check
                result=$(jq -r '.contains_testbed_tag' <<< $value )
                if [[ $result == 'false' ]]; then
                    echo "[***] FAIL: Testbed type marker not found" | tee -a ${{ env.tag_marker_report_path }}
                    rc=1
                fi
                # contains qtest tag check
                result=$(jq -r '.valid_qtest_tag' <<< $value )
                if [[ $result == 'false' ]]; then
                    echo "[***] FAIL: qtest marker invalid or not found" | tee -a ${{ env.tag_marker_report_path }}
                    rc=1
                fi
            done <<< $func
            echo "" | tee -a ${{ env.tag_marker_report_path }}
        done <<< $testcase_files
        exit $rc


    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v2
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.tag_marker_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "::set-output name=status::${{ job.status }}"

  check_tests_functionality:
    needs: [check_tags]
    name: Test functionality validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}

    steps:

    - uses: actions/download-artifact@v2
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        # Create report file
        cat >${{ env.testcase_func_report_path }} <<EOL
        ***********************************************************
        ************** Testcase Functionality Report **************
        ***********************************************************

        EOL

    - name: Pre-job-submission prep
      run: |
        # Get a session token for this run
        session_token=$( curl -X GET  --no-progress-meter \
                        "http://autoiq.extremenetworks.com/auth/getSessionTokenFromPAT" \
                        -H "accept: application/json" \
                        -H "authorization: PAT ${{ secrets.AUTOIQ_PAT }}" \
                        | jq -r '.result.sessionToken' )

        # Get list of available testbeds for each harness
        # harnesses: econ_auto(pytest), xiq_auto(robot), tcl
        # 11111111-2222-2222-2222-111111111111 (pytest), 11111111-6666-6666-6666-111111111111 (robot), 11111111-5555-5555-5555-111111111111 (tcl)
        available_testbeds_pytest=$( curl -X GET --no-progress-meter \
                                    "http://autoiq.extremenetworks.com/tbedmgr/jobmgr/cicdPlatforms/econ_auto" \
                                    -H "accept: application/json" \
                                    -H "authorization: Bearer ${session_token}" \
                                    | jq '.result[]' )

        available_testbeds_robot=$( curl -X GET --no-progress-meter \
                                    "http://autoiq.extremenetworks.com/tbedmgr/jobmgr/cicdPlatforms/xiq_auto" \
                                    -H "accept: application/json" \
                                    -H "authorization: Bearer ${session_token}" \
                                    | jq '.result[]' )

        echo "pytest testbeds: $available_testbeds_pytest"
        echo "robot testbeds: $available_testbeds_robot"

    # # Submit job for each platform on each NOS that runs all the tests

    # # - name: Run Modified TestCases
    # #   run: |
    # curl -X POST --no-progress-meter \
    # "https://autoiq.extremenetworks.com/tbedmgr/jobmgr/createJob" \
    # -H "accept: application/json" \
    # -H "authorization: Bearer ${session_token}" \
    # '{"jsonString":{ \
    #     "description": "CI Verification Run", \
    #     "priority": 3, \
    #     "rerun": 0, \
    #     "postResults": 1, \
    #     "notifyOnStart": 0, \
    #     "username": "CI_Service", \
    #     "group": "CICD", \
    #     "jobType": "private", \
    #     "auxInfoList" : [ \
    #       {"cicd_pr":1}, \
    #       {"context": "econ-cron/test-context"}, \
    #       {"repo":"extremenetworks/econ-cron"}, \
    #       {"sha":"a964180c2aae2fd5c72d1137d9e20b6678217a82"} \
    #     ], \
    #     "jobPlatforms": [ \
    #       { \
    #           "platform": "i386", \
    #           "hardwareConfiguration": "Standalone", \
    #           "priority": 3, \
    #           "auxInfoList": [], \
    #           "jobPlatformTestModules" : \
    #           [ \
    #               { \
    #                   "testModule_uuid": "11111111-2222-2222-2222-111111111111", \
    #                   "auxInfoList" : [ \
    #                                     {"nodeCount": 4}, \
    #                                     {"universalTestBedOs": "EXOS"}, \
    #                                     {"test_name": ["blah.py","Tests\whatever\blah.py", "Tests\"] } \
    #                                     ] \
    #               } \
    #           ] \
    #       } \
    #     ] \
    # }}'

    # robot testbeds: {
    #   "hardwareConfiguration": "Standalone",
    #   "nodeCount": 4,
    #   "platform": "i386",
    #   "universalTestBedOs": ["blah","test"]
    # }

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v2
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.testcase_func_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "::set-output name=status::${{ job.status }}"

  handle_results:
    needs: [check_tests_functionality, check_file, check_names, check_tags, check_testbeds, check_dir]
    name: Send result emails
    if: ${{ always() }}
    runs-on: ubuntu-latest
    env:
      EMAIL_BODY_FILE: body.html

    steps:
    - name: Checkout
      uses: actions/checkout@v2

    - uses: actions/download-artifact@v2
      with:
        name: ${{ env.report_artifact_name }}

    - name: Set environment variables
      run: |
        echo 'github actor: ${{ github.actor }}'
        actor_email=$( jq -r '.${{ github.actor }}' .github/workflows/github_names_to_extr_email.json )
        echo "ACTOR_EMAIL=${actor_email}" >> $GITHUB_ENV

        # Set default status values for email subject line and body
        cat > ${EMAIL_BODY_FILE} << EOL
        <h3>Results:</h3>

        <p>Testbed file validation:            ${{ needs.check_testbeds.outputs.status }}</p>

        <p>File name and location validation:  ${{ needs.check_file.outputs.status }}</p>

        <p>Directory structure validation:     ${{ needs.check_dir.outputs.status }}</p>

        <p>Testcase naming validation:         ${{ needs.check_names.outputs.status }}</p>

        <p>Tags/Markers validation:            ${{ needs.check_tags.outputs.status }}</p>

        <p>Test functionality validation:      ${{ needs.check_tests_functionality.outputs.status }}</p>
        <hr>

        <p>The log files from your run of ${{ github.workflow }} are attached to this email.</p>

        <p>The full results for this run can be seen here: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}.</p>
        EOL

        # Set overall workflow status
        if [[ "${{ needs.check_testbeds.outputs.status }}" == "failure" || "${{ needs.check_file.outputs.status }}" == "failure" || "${{ needs.check_dir.outputs.status }}" == "failure" || "${{ needs.check_names.outputs.status }}" == "failure" || "${{ needs.check_tags.outputs.status }}" == "failure" || "${{ needs.check_tests_functionality.outputs.status }}" == "failure" ]]; then
          echo "WORKFLOW_STATUS=Failed" >> $GITHUB_ENV
        else
          echo "WORKFLOW_STATUS=Passed" >> $GITHUB_ENV
        fi

    - name: Email results
      uses: dawidd6/action-send-mail@v3
      continue-on-error: true
      with:
        # Required mail server address:
        server_address: smtp.office365.com
        # Required mail server port:
        server_port: 587
        # Mail server username:
        username: srv-econ-notify@extremenetworks.com
        # Mail server password:
        password: "[kkwm{Y-36{k"
        # Required mail subject:
        subject: CI Test ${{ env.WORKFLOW_STATUS }} - ${{ github.repository }}
        # Required recipients' addresses:
        to: ${{ env.ACTOR_EMAIL }}
        # Required sender full name (address can be skipped):
        from: srv-econ-notify@extremenetworks.com
        # Optional whether this connection use TLS (default is true if server_port is 465)
        secure: false
        # Optional plain body:
        #body: ${{ env.EMAIL_BODY }}
        # Optional HTML body read from file:
        html_body: file://${{ env.EMAIL_BODY_FILE }}
        # Optional unsigned/invalid certificates allowance:
        ignore_cert: true
        # Optional converting Markdown to HTML (set content_type to text/html too):
        convert_markdown: true
        # Attachments: all text report files in current dir
        attachments: ./*_report.txt
