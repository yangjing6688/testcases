name: CI Workflow
# Ensures only one instance of the CI workflow is running per branch
# This stops multiple rapid commits from holding up the process
concurrency: ci-${{ github.ref }}

on:
  workflow_dispatch:

  pull_request:
    branches: [main, RELEASE-**, 2022_Q1r2]

env:
  fail_prefix:                  '\033[1;31mFAIL -\033[0m'
  error_prefix:                 '\033[1;31mERROR -\033[0m'
  pass_prefix:                  '\033[1;32mPASS -\033[0m'
  pytest_func_path:             pytest_func_tests.txt
  pytest_non_prod_path:         pytest_non_prod_tests.txt
  pytest_libs_path:             pytest_libs.txt
  pytest_res:                   pytest_resources.txt
  robot_func_path:              robot_func_tests.txt
  robot_non_prod_path:          robot_non_prod_tests.txt
  robot_libs_path:              robot_libs.txt
  testbed_prod_path:            testbeds_prod.txt
  testbed_non_prod_path:        testbeds_non_prod.txt
  other_files_path:             other_files.txt
  changed_files_artifact_name:  changed-files
  testbed_files_report_path:    testbed_files_report.txt
  testcase_files_report_path:   testcase_files_report.txt
  directory_struct_report_path: directory_structure_report.txt
  testcase_naming_report_path:  testcase_naming_report.txt
  tag_marker_report_path:       tag_and_marker_report.txt
  # Commented out on 4/21/22 by petersadej.
  # This report is currently broken due to changes outside of this script. All or parts of this can be used in the future
  # testcase_func_report_path:    testcase_functionality_report.txt
  report_artifact_name:         CI-reports-${{ github.event.repository.name }}-run-${{ github.run_number }}
  documentation_url:            https://extremenetworks2com.sharepoint.com/:w:/r/sites/qa-extauto/_layouts/15/Doc.aspx?sourcedoc=%7B357F1F2C-B274-4739-9D87-A67A72875846%7D&file=Automation%20Process%20-%20Draft.docx

jobs:
  check_changes:
    name: Check for changes in test scripts
    runs-on: ubuntu-latest
    # These outputs are used to determine whether certain checks should run
    outputs:
      run-pytest: ${{ steps.pytest-func.outputs.any_changed }}
      run-robot: ${{ steps.robot-func.outputs.any_changed }}

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    # https://github.com/tj-actions/changed-files
    - name: Get changed pytest production tests
      id: pytest-func
      uses: tj-actions/changed-files@v35
      with:
        files: |
          Tests/Pytest/Functional/**/TestCases
          Tests/Pytest/SystemTest/**/TestCases
        separator: ','

    - name: Get changed pytest non-prod tests
      id: pytest-non-prod
      uses: tj-actions/changed-files@v35
      with:
        files: |
          Tests/Pytest/NonProduction/**/TestCases
          Tests/Pytest/ReleaseTest/**/TestCases
        separator: ','

    - name: Get changed pytest resources
      id: pytest-resources
      uses: tj-actions/changed-files@v35
      with:
        files: |
          Tests/Pytest/**/Resources
        separator: ','

    - name: Get changed robot production tests
      id: robot-func
      uses: tj-actions/changed-files@v35
      with:
        files: |
          Tests/Robot/SystemTest/**/TestCases
          Tests/Robot/Functional/**/TestCases
        separator: ','

    - name: Get changed robot non-prod tests
      id: robot-non-prod
      uses: tj-actions/changed-files@v35
      with:
        files: |
          Tests/Robot/NonProduction/**/TestCases/
          Tests/Robot/ReleaseTest/**/TestCases
        separator: ','

    - name: Get changed robot libraries
      id: robot-libs
      uses: tj-actions/changed-files@v35
      with:
        files: |
          Tests/Robot/Libraries
        separator: ','

    - name: Get changed pytest libraries
      id: pytest-libs
      uses: tj-actions/changed-files@v35
      with:
        files: |
          Tests/Pytest/Libraries
        separator: ','

    - name: Get changed Non-Prod Testbed YAML files
      id: testbed-non-prod
      uses: tj-actions/changed-files@v35
      with:
        files: |
          TestBeds/Robot/Demo/**/*.{yaml,yml}
          TestBeds/Robot/Dev/**/*.{yaml,yml}
          TestBeds/Pytest/Demo/**/*.{yaml,yml}
          TestBeds/Pytest/Dev/**/*.{yaml,yml}
          TestBeds/Templates/**/*.{yaml,yml}
        separator: ','

    - name: Get changed Prod Testbed YAML files
      id: testbed-prod
      uses: tj-actions/changed-files@v35
      with:
        files: |
          TestBeds/*/Prod/**/*.{yaml,yml}
          TestBeds/*/SystemsTest/**/*.{yaml,yml}
        separator: ','

    # Outputs a list of files breaking directory rules that we care about
    - name: Get other changed files
      id: other-files
      uses: tj-actions/changed-files@v35
      with:
        files: |
          Tests/Pytest/SystemTest/**/TestCases
          Tests/Pytest/ReleaseTest/**/TestCases
          Tests/Pytest/Functional/**/TestCases
          Tests/Pytest/NonProduction/**/TestCases
          Tests/Pytest/SystemTest/**/Resources
          Tests/Pytest/ReleaseTest/**/Resources
          Tests/Pytest/Functional/**/Resources
          Tests/Pytest/NonProduction/**/Resources
          Tests/Robot/SystemTest/**/TestCases
          Tests/Robot/ReleaseTest/**/TestCases
          Tests/Robot/Functional/**/TestCases
          Tests/Robot/NonProduction/**/TestCases
          Tests/Robot/SystemTest/**/Resources
          Tests/Robot/ReleaseTest/**/Resources
          Tests/Robot/Functional/**/Resources
          Tests/Robot/NonProduction/**/Resources
          Tests/Pytest/Libraries
          Tests/Robot/Libraries
          Tests/Pytest/Demos
          Tests/Robot/Demos
          **/__init__.py
          **/conftest.py
          **/.pylintrc
          **/.gitignore
          **/{r,R}{e,E}{a,A}{d,D}{m,M}{e,E}.md
          data
          Environments
          TestBeds
          Utils
        separator: ','

    - name: List all modified files
      env:
        PY_FUNC:          ${{ steps.pytest-func.outputs.all_changed_files }}
        PY_NON_PROD:      ${{ steps.pytest-non-prod.outputs.all_changed_files }}
        PY_LIBS:          ${{ steps.pytest-libs.outputs.all_changed_files }}
        PY_RES:           ${{ steps.pytest-resources.outputs.all_changed_files }}
        ROBOT_FUNC:       ${{ steps.robot-func.outputs.all_changed_files }}
        ROBOT_NON_PROD:   ${{ steps.robot-non-prod.outputs.all_changed_files }}
        ROBOT_LIBS:       ${{ steps.robot-libs.outputs.all_changed_files }}
        TESTBED_PROD:     ${{ steps.testbed-prod.outputs.all_changed_files }}
        TESTBED_NON_PROD: ${{ steps.testbed-non-prod.outputs.all_changed_files }}
        OTHER_FILES:      ${{ steps.other-files.outputs.other_changed_files }}
      run: |
        echo $PY_FUNC > ${{ env.pytest_func_path }}
        echo $PY_NON_PROD > ${{ env.pytest_non_prod_path }}
        echo $PY_LIBS > ${{ env.pytest_libs_path }}
        echo $PY_RES > ${{ env.pytest_res }}
        echo $ROBOT_FUNC > ${{ env.robot_func_path }}
        echo $ROBOT_NON_PROD > ${{ env.robot_non_prod_path }}
        echo $ROBOT_LIBS > ${{ env.robot_libs_path }}
        echo $TESTBED_PROD > ${{ env.testbed_prod_path }}
        echo $TESTBED_NON_PROD > ${{ env.testbed_non_prod_path }}
        echo $OTHER_FILES > ${{ env.other_files_path }}


    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v3
      with:
        name: ${{ env.changed_files_artifact_name }}
        path: |
          ${{ env.pytest_func_path }}
          ${{ env.pytest_non_prod_path }}
          ${{ env.pytest_libs_path }}
          ${{ env.pytest_res }}
          ${{ env.robot_func_path }}
          ${{ env.robot_non_prod_path }}
          ${{ env.robot_libs_path }}
          ${{ env.testbed_prod_path }}
          ${{ env.testbed_non_prod_path }}
          ${{ env.other_files_path }}

  check_testbeds:
    needs: [check_changes]
    name: Testbed file validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - uses: actions/download-artifact@v3
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        # Create report file
        cat >${{ env.testbed_files_report_path }} <<EOL
        ******************************************************
        ************** Testbed YAML File Report **************
        ******************************************************

        [*] Checking production testbed files...

        EOL

    - name: Handle Skip Requests
      run: |
        python .github/workflows/skip_handler.py --actor "${{ github.actor }}" --title '${{ github.event.pull_request.title }}' --check testbed | tee -a ${{ env.testbed_files_report_path }}

    - name: Check production test bed files
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        set -o pipefail # Preserves the return code from the python script in pipeline below
        python .github/workflows/testbed_yaml_parser.py ${{ env.testbed_prod_path }} | tee -a ${{ env.testbed_files_report_path }}

    - name: Check non-production test bed files
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        printf '%b\n' "[*] Checking non-production testbed files..."
        python .github/workflows/testbed_yaml_parser.py ${{ env.testbed_non_prod_path }} | tee -a ${{ env.testbed_files_report_path }}

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v3
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.testbed_files_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "status=${{ job.status }}" >> $GITHUB_OUTPUT

  check_file:
    needs: [check_changes]
    name: File name and location validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}

    steps:
    # For skip request processing
    - name: Checkout
      uses: actions/checkout@v3

    - uses: actions/download-artifact@v3
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        # Create report file
        cat >${{ env.testcase_files_report_path }} <<EOL
        *****************************************************
        ********** Robot/PyTest file Naming Report **********
        *****************************************************

        EOL

    - name: Handle Skip Requests
      run: |
        python .github/workflows/skip_handler.py --actor "${{ github.actor }}" --title '${{ github.event.pull_request.title }}' --check file | tee -a ${{ env.testcase_files_report_path }}

    - name: Process Pytest Production file(s)
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for path in $( cat ${{ env.pytest_func_path }} | sed -e 's/ //g' ); do
          # Only allow __init__.py or files that conform to "test_<testname>_<valid_jira_id>.py"
          # valid jira id: One or more letters followed by one or more numbers. A dash between the letters and numbers is optional.
          # This can potentially be followed by _ and 1 or more numbers if multiple testcase files are needed for the same id.
          # Examples: test_blah_app-45   test_blah_blah_app45_1   test_blah_app-45_1   test_blah_app45
          # Note: inside bash [[ ]] you need to use extended regex format

          errors=0
          f=$( basename "$path" )
          let "files_tested += 1"
          printf '\n%b\n' "[*] Processing Pytest file: $path ..." | tee -a ${{ env.testcase_files_report_path }}

          # Test for .py file extension
          if [[ ! $f =~ .*\.py$ ]] ; then
              printf '%b\n' "[**] ${fail_prefix} $path is NOT a Pytest file!  Only .py files are allowed in this directory." | tee -a ${{ env.testcase_files_report_path }}
              rc=1; let "errors += 1"
          fi

          # Test for "test_" prefix. Ignore __init__.py, conftest.py, .pylintrc files
          if [[ ! $f =~ ^test_.*\.py$|__init__\.py$|conftest\.py$|\.pylintrc$ ]] ; then
              printf '%b\n' "[**] ${fail_prefix} $path is missing the \"test_\" prefix!" | tee -a ${{ env.testcase_files_report_path }}
              rc=1; let "errors += 1"
          fi

          # Check for valid Jira ID suffix. Ignore __init__.py, conftest.py, .pylintrc files
          if [[ ! $f =~ ^test_.*_[a-zA-Z]+[\-_]?[0-9]+(_[0-9]+)?\.py$|__init__\.py$|conftest\.py$|\.pylintrc$ ]] ; then
              printf '%b\n' "[**] ${fail_prefix} $path does not contain a valid Jira ID suffix!" | tee -a ${{ env.testcase_files_report_path }}
              rc=1; let "errors += 1"
          fi

          if (( errors == 0 )); then
              printf '%b\n' "[**] ${pass_prefix} $path" | tee -a ${{ env.testcase_files_report_path }}
          else
              printf '%b\n' '[**] For more information see the Test Suite File Name section of ${{ env.documentation_url }}' | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          printf '%b\n' "[*] No Pytest Production files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Process Pytest Non-Production file(s)
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for path in $( cat ${{ env.pytest_non_prod_path }} | sed -e 's/ //g' ); do
          errors=0
          f=$( basename "$path" )
          let "files_tested += 1"
          printf '\n%b\n' "[*] Processing Pytest file: $path ..." | tee -a ${{ env.testcase_files_report_path }}
          # Test for .py file extension
          if [[ ! $f =~ .*\.py$ ]] ; then
              printf '%b\n' "[**] ${fail_prefix} $path is NOT a Pytest file! Only .py files are allowed in this directory." | tee -a ${{ env.testcase_files_report_path }}
              rc=1; let "errors += 1"
          fi

          # Only allow files that conform to "test_<testname>.py". Ignore __init__.py, conftest.py, .pylintrc files
          # Note: inside bash [[ ]] you need to use extended regex format
          if [[ ! "$f" =~ ^test_.*\.py$|__init__\.py$|conftest\.py$|\.pylintrc$ ]] ; then
              printf '%b\n' "[**] ${fail_prefix} $path is missing the \"test_\" prefix!" | tee -a ${{ env.testcase_files_report_path }}
              rc=1; let "errors += 1"
          fi

          if (( errors == 0 )); then
              printf '%b\n' "[**] ${pass_prefix} $path" | tee -a ${{ env.testcase_files_report_path }}
          else
              printf '%b\n' '[**] For more information see the Test Suite File Name section of ${{ env.documentation_url }}' | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          printf '%b\n' "[*] No Pytest Non-Production files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Process Pytest Library file(s)
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for path in $( cat ${{ env.pytest_libs_path }} | sed -e 's/ //g' ); do
          f=$( basename "$path" )
          let "files_tested += 1"
          printf '\n%b\n' "[*] Processing Python file: $path ..." | tee -a ${{ env.testcase_files_report_path }}
          if [[ ! $f =~ .*\.py$ ]] ; then
              printf '%b\n' "[**] ${fail_prefix} $path is NOT a Python file! Only .py files are allowed in this directory." | tee -a ${{ env.testcase_files_report_path }}
              rc=1;
          else
              printf '%b\n' "[**] ${pass_prefix} $path" | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          printf '%b\n' "[*] No Pytest Library files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Process Robot Production file(s)
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for path in $( cat ${{ env.robot_func_path }} | sed -e 's/ //g' ); do
          # Only allow files that conform to "<testname>_<valid_jira_id>.robot" or "__init__.robot"
          # See "Process Pytest Production file(s)" for definition of <valid_jira_id>

          errors=0
          f=$( basename "$path" )
          let "files_tested += 1"
          printf '\n%b\n' "[*] Processing Robot file: $path ..." | tee -a ${{ env.testcase_files_report_path }}

          # Test for .robot file extension
          if [[ ! $f =~ .*\.robot$ ]] ; then
              printf '%b\n' "[**] ${fail_prefix} $path is NOT a Robot file! Only .robot files are allowed in this directory." | tee -a ${{ env.testcase_files_report_path }}
              rc=1; let "errors += 1"
          fi

          # Check for Jira ID suffix. Ignore __init__.robot files
          if [[ ! $f =~ _[a-zA-Z]+[\-_]?[0-9]+(_[0-9]+)?\.robot$|__init__\.robot$ ]] ; then
              printf '%b\n' "[**] ${fail_prefix} $path does not contain a valid Jira ID suffix!" | tee -a ${{ env.testcase_files_report_path }}
              rc=1; let "errors += 1"
          fi

          if (( errors == 0 )); then
              printf '%b\n' "[**] ${pass_prefix} $path" | tee -a ${{ env.testcase_files_report_path }}
          else
              printf '%b\n' '[**] For more information see the Test Suite File Name section of ${{ env.documentation_url }}' | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          printf '%b\n' "[*] No Robot Production files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Process Robot Non-Production file(s)
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for path in $( cat ${{ env.robot_non_prod_path }} | sed -e 's/ //g' ); do
          f=$( basename "$path" )
          let "files_tested += 1"
          printf '\n%b\n' "[*] Processing Robot file: $path ..." | tee -a ${{ env.testcase_files_report_path }}
          if [[ ! $f =~ .*\.robot$ ]] ; then
              printf '%b\n' "[**] ${fail_prefix} $path is NOT a Robot file! Only .robot files are allowed in this directory." | tee -a ${{ env.testcase_files_report_path }}
              rc=1
          else
              printf '%b\n' "[**] ${pass_prefix} $path" | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          printf '%b\n' "[*] No Robot Non-Production files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Process Robot Library file(s)
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        IFS=','
        rc=0
        files_tested=0
        for path in $( cat ${{ env.robot_libs_path }} | sed -e 's/ //g' ); do
          f=$( basename "$path" )
          let "files_tested += 1"
          printf '\n%b\n' "[*] Processing Robot file: $path ..." | tee -a ${{ env.testcase_files_report_path }}
          if [[ ! $f =~ .*\.robot$ ]] ; then
              printf '%b\n' "[**] ${fail_prefix} $path is NOT a Robot file! Only .robot files are allowed in this directory." | tee -a ${{ env.testcase_files_report_path }}
              rc=1
          else
              printf '%b\n' "[**] ${pass_prefix} $path" | tee -a ${{ env.testcase_files_report_path }}
          fi
        done
        unset IFS
        if (( files_tested == 0 )); then
          printf '%b\n' "[*] No Robot Library files found. Skipping these tests..." | tee -a ${{ env.testcase_files_report_path }}
        fi
        exit $rc

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v3
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.testcase_files_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "status=${{ job.status }}" >> $GITHUB_OUTPUT

  check_dir:
    needs: [check_changes]
    name: Directory structure validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - uses: actions/download-artifact@v3
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        # Create report file
        cat >${{ env.directory_struct_report_path }} <<EOL
        ********************************************************
        ************** Directory Structure Report **************
        ********************************************************

        EOL

    - name: Handle Skip Requests
      run: |
        python .github/workflows/skip_handler.py --actor "${{ github.actor }}" --title '${{ github.event.pull_request.title }}' --check dir | tee -a ${{ env.directory_struct_report_path }}

    - name: Check if any disallowed directories were created
      if: ${{env.SKIP_CHECK == 'false'}}
      run: |
        trimmed_files=$(cat ${{ env.other_files_path }} | tr -d [:space:])
        if [[ $trimmed_files ]]; then
          printf '%b\n' "[*] ${fail_prefix} New directories and files not allowed in protected areas." | tee -a ${{ env.directory_struct_report_path }}
          printf '%b\n' "[*] Common reasons for seeing this error:" | tee -a ${{ env.directory_struct_report_path }}
          printf '%b\n' "[*]   - Adding files in /Tests outside of TestCases or Resource directories. (Except: __init__.py, .pylintrc, readme.md)" | tee -a ${{ env.directory_struct_report_path }}
          printf '%b\n' "[*]   - Adding directories/files in /Tests outisde of /Tests/<framework>/<test type>/ " | tee -a ${{ env.directory_struct_report_path }}
          printf '%b\n' "[*]   - Adding directories/files outisde of /Tests, /data, /Environments, /TestBeds, /Utils " | tee -a ${{ env.directory_struct_report_path }}
          printf '%b\n' '[*] For more information see the Test Suite Location section of ${{ env.documentation_url }}' | tee -a ${{ env.directory_struct_report_path }}
          printf '%b\n' "[*] Offending files/directories:" | tee -a ${{ env.directory_struct_report_path }}

          IFS=','
          for path in $trimmed_files ; do
            printf '%b\n' "[**] $path" | tee -a ${{ env.directory_struct_report_path }}
          done
          exit 1
        else
          printf '%b\n' "[*] ${pass_prefix} No files/directories found in protected areas." | tee -a ${{ env.directory_struct_report_path }}
        fi

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v3
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.directory_struct_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "status=${{ job.status }}" >> $GITHUB_OUTPUT

  check_names:
    needs: [check_file, check_dir]
    name: Testcase naming validation
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.set-output.outputs.status }}

    steps:
    - name: Checkout modified files
      uses: actions/checkout@v3

    - uses: actions/download-artifact@v3
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        cat >${{ env.testcase_naming_report_path }} <<EOL
        ******************************************************
        *************** Testcase Naming Report ***************
        ******************************************************

        EOL

    # Checks for testcases being mistakenly put in the resources directory
    - name: Process modified Pytest Resources files
      run: |
        files_tested=0
        rc=0
        for path in $( cat ${{ env.pytest_res }} | tr ',' '\n' ) ; do
            filename=$( basename "$path" )
            let "files_tested += 1"
            printf '\n%b\n' "[*] Processing Pytest Resources file: $path ..." | tee -a ${{ env.testcase_naming_report_path }}

            # Skip __init__.py, conftest.py, .pylintrc files
            #
            if [[ ${filename} =~ __init__\.py$|conftest\.py$|\.pylintrc$ ]] ; then
                printf '%b\n' "[**] ${pass_prefix} File: ${path}" | tee -a ${{ env.testcase_naming_report_path }}
                continue
            fi

            for func in $( grep -w def $path | awk '{print $2}' ); do
                printf '%b\n' "Testing Function: ${func} ..." | tee -a ${{ env.testcase_naming_report_path }}

                if [[ ${func} =~ ^test_.*$ ]] ; then
                  printf '%b\n' "[**] ${fail_prefix} Function: ${func}" | tee -a ${{ env.testcase_naming_report_path }}
                  rc=1
                else
                  printf '%b\n' "[**] ${pass_prefix} Function: ${func}" | tee -a ${{ env.testcase_naming_report_path }}
                fi
            done
        done
        if (( files_tested == 0 )); then
          printf '%b\n' "[*] No Pytest Resource files found. Skipping these tests..." | tee -a ${{ env.testcase_naming_report_path }}
        fi
        exit $rc

    # Checks for non-testcases in testcase directory
    - name: Process modified Pytest Test files
      run: |
        files_tested=0
        rc=0
        for path in $( cat ${{ env.pytest_func_path }} ${{ env.pytest_non_prod_path }} | tr ',' '\n' ) ; do
            filename=$( basename "$path" )
            let "files_tested += 1"
            printf '\n%b\n' "[*] Processing Pytest Test file: $path ..." | tee -a ${{ env.testcase_naming_report_path }}

            # Skip __init__.py, conftest.py, .pylintrc files
            #
            if [[ ${filename} =~ __init__\.py$|conftest\.py$|\.pylintrc$ ]] ; then
                printf '%b\n' "[**] ${pass_prefix} File: ${path}" | tee -a ${{ env.testcase_naming_report_path }}
                continue
            fi

            func=$( grep -w def $path | awk '{print $2}' | grep ^test_ )
            if [[ ${func-} ]] ; then
              printf '%b\n' "[**] ${pass_prefix} File: ${path}" | tee -a ${{ env.testcase_naming_report_path }}
            else
              printf '%b\n' "[**] ${fail_prefix} File: ${path}" | tee -a ${{ env.testcase_naming_report_path }}
              rc=1
            fi
        done
        if (( files_tested == 0 )); then
          printf '%b\n' "[*] No Pytest Test files found. Skipping these tests..." | tee -a ${{ env.testcase_naming_report_path }}
        fi
        exit $rc

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v3
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.testcase_naming_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "status=${{ job.status }}" >> $GITHUB_OUTPUT

  check_tags:
    needs: [check_changes, check_names]
    name: Tags/Markers validation
    # runs-on: ubuntu-latest
    runs-on: Ubuntu-20.04
    outputs:
      status: ${{ steps.set-output.outputs.status }}
    env:
      pytest_result_file: pytest_data.json
      robot_result_file: robot_data.json

    steps:
    - uses: actions/download-artifact@v3
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Prep environment
      run: |
        # Create report file
        cat >${{ env.tag_marker_report_path }} <<EOL
        ********************************************************
        ************** Testcase Tag/Marker Report **************
        ********************************************************

        EOL


        # Set env vars
        echo "RUN_PYTEST=${{ needs.check_changes.outputs.run-pytest }}" >> $GITHUB_ENV
        echo "RUN_ROBOT=${{ needs.check_changes.outputs.run-robot }}" >> $GITHUB_ENV

    # Only do the below steps if we need to evaluate any of the changes being committed
    - name: Checkout local repo
      if: ${{env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true'}}
      uses: actions/checkout@v3

    - name: Handle Skip Requests
      if: ${{env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true'}}
      run: |
        # Handle skipping entire Tags section
        #
        python .github/workflows/skip_handler.py --actor "${{ github.actor }}" --title '${{ github.event.pull_request.title }}' --check tags | tee -a ${{ env.tag_marker_report_path }}

        echo "SKIP_RESERVED_TAGS=true" >> $GITHUB_ENV
        printf '\n%s\n%s\n' "====== Shell environment variables ======" "$(env)"
        printf '\n%s\n%s\n' "====== GitHub environment variables ======" "$(cat $GITHUB_ENV)"
    ### 11/29/22 by peter sadej
    ### The reserved tags check has become cumbersome for our users. Always skip the check until we make a decision on if or how it should work in the future
    ### # Handle skipping entire only reserved tags checks (sanity, p1, p2, etc.)
    ### #
    ### python .github/workflows/skip_handler.py --actor "${{ github.actor }}" --title '${{ github.event.pull_request.title }}' \
    ###     --check "reserved tags" --extended_auth "tags" --envvar "SKIP_RESERVED_TAGS" | tee -a ${{ env.tag_marker_report_path }}

    - name: Checkout framework
      if: ${{env.SKIP_CHECK == 'false' && (env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true')}}
      uses: actions/checkout@v3
      with:
        repository: extremenetworks/extreme_automation_framework
        token: ${{ secrets.REGISTRY_PAT }}
        path: extreme_automation_framework
        ref: main

    - uses: actions/download-artifact@v3
      if: ${{env.SKIP_CHECK == 'false' && (env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true')}}
      with:
        name: ${{ env.changed_files_artifact_name }}

    - name: Install python requirements
      if: ${{env.SKIP_CHECK == 'false' && (env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true')}}
      run: |
        pip install -r requirements.txt

    - name: Run pytest inventory tool
      if: ${{env.SKIP_CHECK == 'false' && env.RUN_PYTEST == 'true'}}
      run: |
        # Add framework to PYTHONPATH
        export PYTHONPATH="${PYTHONPATH}:${PWD}/extreme_automation_framework"

        pytest_files=$(cat ${{ env.pytest_func_path }} | tr ',\n' ' ')

        # Run pytest
        pytest --get_test_info cicd ${pytest_files} || true

        # Check if inventory tool had problems
        if [[ ! -f "${{ env.pytest_result_file }}" ]]; then
            printf '%b\n' "${error_prefix} output file not found. Check above exception."
            exit 1
        fi

    - name: Run robot inventory tool
      if: ${{env.SKIP_CHECK == 'false' && env.RUN_ROBOT == 'true'}}
      env:
        ROBOT_TOOL_PATH: Tests/Robot/get_test_info.py
      run: |
        # Add framework to PYTHONPATH
        export PYTHONPATH="${PYTHONPATH}:${PWD}/extreme_automation_framework"

        robot_files=$(cat ${{ env.robot_func_path }} | tr ',\n' ' ')

        # Run robot
        python $ROBOT_TOOL_PATH ${robot_files}

        # Check if inventory tool had problems
        if [[ ! -f "${{ env.robot_result_file }}" ]]; then
            printf '%b\n' "${error_prefix} output file not found. Check above exception."
            exit 1
        fi

    - name: Check pytest markers
      if: ${{env.SKIP_CHECK == 'false' && env.RUN_PYTEST == 'true'}}
      run: |
        set -o pipefail # Preserves the return code from the python script in pipeline below
        python .github/workflows/testcase_tags_parser.py ${{ env.pytest_result_file }} --mode validate_tags \
          --framework pytest --skip_reserved_tags ${SKIP_RESERVED_TAGS} --auth_token ${{ secrets.AUTOIQ_PAT }} \
          | tee -a ${{ env.tag_marker_report_path }}

    - name: Check robot tags
      if: ${{env.SKIP_CHECK == 'false' && env.RUN_ROBOT == 'true'}}
      run: |
        set -o pipefail # Preserves the return code from the python script in pipeline below
        python .github/workflows/testcase_tags_parser.py ${{ env.robot_result_file }} --mode validate_tags \
          --framework robot --skip_reserved_tags ${SKIP_RESERVED_TAGS} --auth_token ${{ secrets.AUTOIQ_PAT }} \
          | tee -a ${{ env.tag_marker_report_path }}

    - name: Upload test results to GH artifacts
      uses: actions/upload-artifact@v3
      if: ${{ always() }}
      with:
        name: ${{ env.report_artifact_name }}
        path: |
          ${{ env.tag_marker_report_path }}

    - name: Set job status output
      if: ${{ always() }}
      id: set-output
      run: |
        echo "status=${{ job.status }}" >> $GITHUB_OUTPUT

  # ####
  # # Commentted out on 8/15/22 by peter sadej
  # # We decided that coming up with a system that figures out what testbed a testcase needs to run is not worth the effort currently.
  # # Leaving code here incase we come back to this in the future
  # ####

  # check_tests_functionality:
  #   needs: [check_changes, check_tags]
  #   name: Test functionality validation
  #   runs-on: ubuntu-latest
  #   outputs:
  #     status: ${{ steps.set-output.outputs.status }}

  #   steps:
  #   - name: Checkout modified files
  #     uses: actions/checkout@v3

  #   - uses: actions/download-artifact@v3
  #     with:
  #       name: ${{ env.changed_files_artifact_name }}

  #   - name: Prep environment
  #     run: |
  #       echo "RUN_PYTEST=${{ needs.check_changes.outputs.run-pytest }}" >> $GITHUB_ENV
  #       echo "RUN_ROBOT=${{ needs.check_changes.outputs.run-robot }}" >> $GITHUB_ENV


  #       # Create report file
  #       cat >${{ env.testcase_func_report_path }} <<EOL
  #       ***********************************************************
  #       ************** Testcase Functionality Report **************
  #       ***********************************************************

  #       EOL

  #   - name: Handle Skip Requests
  #     run: |
  #       python .github/workflows/skip_handler.py --actor "${{ github.actor }}" --title '${{ github.event.pull_request.title }}' --check func | tee -a ${{ env.testcase_func_report_path }}

  #   - name: Pre-job-submission prep
  #     if: ${{env.SKIP_CHECK == 'false' && (env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true')}}
  #     run: |
  #       # TODO: get this function working
  #       ################################
  #       # run_curl () {
  #       #   echo $1
  #       #   echo "$1"
  #       #   curl_output=$( curl $1 )
  #       #   curl_return_code=$?

  #       #   if (( $curl_return_code != 0 )); then
  #       #     echo "Error contacting to AutoIQ. Message: ${curl_output}"
  #       #     exit 1
  #       #   fi
  #       # }

  #       # # Get a session token for this run
  #       # curl_command="-X GET --no-progress-meter \"http://autoiq.extremenetworks.com/auth/getSessionTokenFromPAT\" -H \"accept: application/json\" -H \"authorization: PAT ${{ secrets.AUTOIQ_PAT }}\""
  #       # echo $curl_command
  #       # run_curl "$curl_command"
  #       # session_token=$( echo "$curl_command" | jq -r '.result.sessionToken' )
  #       # echo $curl_output
  #       ################################

  #       ### Get Session token
  #       curl_return_code=0
  #       curl_output=$( curl -X GET  --no-progress-meter \
  #                       "http://autoiq.extremenetworks.com/auth/getSessionTokenFromPAT" \
  #                       -H "accept: application/json" \
  #                       -H "authorization: PAT ${{ secrets.AUTOIQ_PAT }}" )
  #       curl_return_code=$?

  #       if (( $curl_return_code != 0 )); then
  #         echo "Error getting session token. Message: ${curl_output}"
  #         exit 1
  #       else
  #         session_token=$( echo "${curl_output}" |  jq -r '.result.sessionToken' )
  #       fi

  #       ### Get list of available testbeds for each harness
  #       # harnesses: pytest, robot, tcl
  #       # 11111111-2222-2222-2222-111111111111 (pytest), 11111111-6666-6666-6666-111111111111 (robot), 11111111-5555-5555-5555-111111111111 (tcl)
  #       curl_return_code=0
  #       curl_output=$( curl -X GET --no-progress-meter \
  #                                   "http://autoiq.extremenetworks.com/tbedmgr/jobmgr/cicdPlatforms/pytest" \
  #                                   -H "accept: application/json" \
  #                                   -H "authorization: Bearer ${session_token}" )
  #       curl_return_code=$?

  #       if (( $curl_return_code != 0 )); then
  #         echo "Error getting session token. Message: ${curl_output}"
  #         exit 1
  #       else
  #         available_testbeds_pytest=$( echo "${curl_output}" | jq '.result[]' )
  #       fi

  #       curl_return_code=0
  #       curl_output=$( curl -X GET --no-progress-meter \
  #                                   "http://autoiq.extremenetworks.com/tbedmgr/jobmgr/cicdPlatforms/robot" \
  #                                   -H "accept: application/json" \
  #                                   -H "authorization: Bearer ${session_token}" )
  #       curl_return_code=$?

  #       if (( $curl_return_code != 0 )); then
  #         echo "Error getting session token. Message: ${curl_output}"
  #         exit 1
  #       else
  #         available_testbeds_robot=$( echo "${curl_output}" | jq '.result[]' )
  #       fi

  #       echo "pytest testbeds: $available_testbeds_pytest"
  #       echo "robot testbeds: $available_testbeds_robot"

  #       # echo "ROBOT_TESTBEDS=${available_testbeds_robot}" >> $GITHUB_ENV
  #       # echo "PYTEST_TESTBEDS=${available_testbeds_pytest}" >> $GITHUB_ENV
  #       echo "SESSION_TOKEN=${session_token}" >> $GITHUB_ENV


  #   # Submit job for each platform on each NOS that runs all the tests
  #   #
  #   - name: Run Modified TestCases
  #     if: ${{env.SKIP_CHECK == 'false' && (env.RUN_PYTEST == 'true' || env.RUN_ROBOT == 'true')}}
  #     run: |
  #       # CreateJob body template
  #       job_template='{
  #               "jsonString":{
  #                   "description": "CI Verification Run",
  #                   "priority": 3,
  #                   "version": 1,
  #                   "rerun": 0,
  #                   "postResults": 1,
  #                   "notifyOnStart": 0,
  #                   "username": "CI_Service",
  #                   "group": "CICD",
  #                   "jobType": "private",
  #                   "auxInfoList" : [
  #                     {"cicd_pr":1},
  #                     {"context": "${{ github.event.repository.name }}/test-context"},
  #                     {"repo":"${{ github.repository }}"},
  #                     {"sha":"${{ github.sha }}"},
  #                     {"state":"pending"}
  #                   ],
  #                   "jobPlatforms": [
  #                     {
  #                       "platform": "EXOS-DT",
  #                       "hardwareConfiguration": "Standalone",
  #                       "priority": 3,
  #                       "auxInfoList": [],
  #                       "jobPlatformTestModules" :
  #                       [
  #                           {
  #                               "testModule_uuid": $harness_uuid,
  #                               "auxInfoList" : [
  #                                                 {"nodeCount": 4},
  #                                                 {"universalTestBedOs": "EXOS"},
  #                                                 {"test_name": $tests }
  #                                                 ]
  #                           },
  #                           {
  #                               "testModule_uuid": $harness_uuid,
  #                               "auxInfoList" : [
  #                                                 {"nodeCount": 4},
  #                                                 {"universalTestBedOs": "VOSS"},
  #                                                 {"test_name": $tests }
  #                                                 ]
  #                           }
  #                       ]
  #                     }
  #                   ]
  #               }
  #             }'

  #       # Format Test Lists and add "extreme_automation_tests" prefix to each path
  #       pytest_tests=$( cat ${pytest_func_path} | jq -ncR 'inputs | split(",") | [ .[] | "extreme_automation_tests/" + . ]' )
  #       robot_tests=$( cat ${robot_func_path} | jq -ncR 'inputs | split(",") | [ .[] | "extreme_automation_tests/" + . ]' )

  #       pytest_harness_uuid="11111111-2222-2222-2222-111111111111"
  #       robot_harness_uuid="11111111-6666-6666-6666-111111111111"

  #       printf 'Pytest tests to run: %s\nRobot tests to run: %s\n' "$pytest_tests" "$robot_tests"

  #   if [[ "${{ env.RUN_ROBOT }}" == "true" ]]; then

  #       # Generate robot JSON body
  #       CURL_BODY=$(jq --null-input \
  #                 --argjson tests "$robot_tests" \
  #                 --arg harness_uuid "$robot_harness_uuid" \
  #                 "$job_template")

  #       printf '%s\n\n' "[*] Robot Job Submit JSON: $CURL_BODY" | tee -a ${{ env.testcase_func_report_path }}

  #       curl -X POST --no-progress-meter \
  #       "https://autoiq.extremenetworks.com/tbedmgr/jobmgr/createJob" \
  #       -H "accept: application/json" \
  #       -H "Content-Type: application/json" \
  #       -H "authorization: Bearer ${SESSION_TOKEN}" \
  #       -d "${CURL_BODY}"
  #   fi

  #   if [[ "${{ env.RUN_PYTEST }}" == "true" ]]; then

  #       # Generate pytest JSON body
  #       CURL_BODY=$(jq --null-input \
  #                 --argjson tests "$pytest_tests" \
  #                 --arg harness_uuid "$pytest_harness_uuid" \
  #                 "$job_template")

  #       printf '%s\n\n' "[*] Pytest Job Submit JSON: $CURL_BODY" | tee -a ${{ env.testcase_func_report_path }}

  #       curl -X POST --no-progress-meter \
  #       "https://autoiq.extremenetworks.com/tbedmgr/jobmgr/createJob" \
  #       -H "accept: application/json" \
  #       -H "Content-Type: application/json" \
  #       -H "authorization: Bearer ${SESSION_TOKEN}" \
  #       -d "${CURL_BODY}"
  #   fi


  #   - name: Upload test results to GH artifacts
  #     uses: actions/upload-artifact@v3
  #     if: ${{ always() }}
  #     with:
  #       name: ${{ env.report_artifact_name }}
  #       path: |
  #         ${{ env.testcase_func_report_path }}

  #   - name: Set job status output
  #     if: ${{ always() }}
  #     id: set-output
  #     run: |
  #       echo "status=${{ job.status }}" >> $GITHUB_OUTPUT

  handle_results:
    needs: [check_testbeds, check_file, check_names, check_tags, check_dir] #, check_tests_functionality]
    name: Send result emails
    if: ${{ always() }}
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - uses: actions/download-artifact@v3
      with:
        name: ${{ env.report_artifact_name }}

    - name: Email results
      continue-on-error: true
      run: |
        echo 'github actor: ${{ github.actor }}'
        ACTOR_EMAIL=$( jq -r '."${{ github.actor }}"' .github/workflows/github_names_to_extr_email.json )

        # Set up body
        EMAIL_BODY=$(cat << END_LINE
        <h3>Results:</h3>

        <p>Testbed file validation:            ${{ needs.check_testbeds.outputs.status }}</p>

        <p>File name and location validation:  ${{ needs.check_file.outputs.status }}</p>

        <p>Directory structure validation:     ${{ needs.check_dir.outputs.status }}</p>

        <p>Testcase naming validation:         ${{ needs.check_names.outputs.status }}</p>

        <p>Tags/Markers validation:            ${{ needs.check_tags.outputs.status }}</p>

        <hr>

        <p>The log files from your run of ${{ github.workflow }} are attached to this email.</p>

        <p>The full results for this run can be seen here: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}.</p>
        END_LINE
        )

        # Commentted out on 8/15/22 by peter sadej
        # We decided that coming up with a system that figures out what testbed a testcase needs to run is not worth the effort currently.
        # Leaving code here incase we come back to this in the future
        # <p>Test functionality validation:      ${{ needs.check_tests_functionality.outputs.status }}</p>

        # Set overall workflow status
        if [[ "${{ needs.check_testbeds.outputs.status }}" == "failure" || "${{ needs.check_file.outputs.status }}" == "failure" || "${{ needs.check_dir.outputs.status }}" == "failure" || "${{ needs.check_names.outputs.status }}" == "failure" || "${{ needs.check_tags.outputs.status }}" == "failure" ]]; then
            # Commentted out on 8/15/22 by peter sadej
            # We decided that coming up with a system that figures out what testbed a testcase needs to run is not worth the effort currently.
            # Leaving code here incase we come back to this in the future
            # || "${{ needs.check_tests_functionality.outputs.status }}" == "failure" ]]; then
          WORKFLOW_STATUS="Failed"
        else
          WORKFLOW_STATUS="Passed"
        fi

        SUBJECT="CI Test ${WORKFLOW_STATUS} - ${{ github.repository }}"
        ATTACHMENTS=./*_report.txt

        # Send email
        python .github/workflows/send_email.py --recipients "${ACTOR_EMAIL}" --subject "${SUBJECT}" --message "${EMAIL_BODY}" --attachments ${ATTACHMENTS} --auth-token "${{ secrets.AUTOIQ_PAT }}"
