name: Update Test Infomation in S3 for the UI
on:
  workflow_dispatch:
    inputs:
      dry-run:
        description: 'Dry run? (Only accepts "true" or "false")'
        required: false
        default: true
  # Only the clean up job runs on this trigger
  schedule:
      - cron: '* 0 * * *'   # Run once at the beginning of every day
  # create and delete triggers are used for updating the lists of branches and tags
  create:
  delete:
  # push trigger is used for updating the Testbed yaml files and private branch test case infomation
  push:

env:
  WORK_DIR: workflow_output

jobs:
  update_testbed_files:
    name: Update Test Bed Info
    runs-on: ubuntu-latest
    # workflow_dispatch is a manual trigger. It would only get used for testing or other extenuating circumstances
    # This job's normal trigger is every push to main.
    # Tiggering in this fashion ensures the testbed YAML files in S3 are always up to date with production files in main
    if: ${{ github.event_name == 'workflow_dispatch' || ( github.event_name == 'push' && github.ref_name == 'main' ) }}

    steps:
    - name: Checkout
      uses: actions/checkout@v3
      with:
        fetch-depth: 0
        ref: main

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-1

    - name: Get changed Testbed YAML files
      id: testbed
      uses: tj-actions/changed-files@v35
      with:
        files: |
          TestBeds
        files_ignore: |
          Testbeds/Templates

    - name: Upload Testbed files
      run: |
        if [[ "${{ steps.testbed.outputs.any_modified }}" == "true" || "${{ github.event_name }}" == "workflow_dispatch" ]]; then
          echo "[*] Found changed testbed files. Uploading..."
          aws s3 sync TestBeds s3://econgit/extreme_automation_tests/TestBeds --exclude "*" --include "*/Prod/*.yaml"
        else
          echo "[*] No testbed files changed. Skipping upload..."
        fi

  cleanup_branches:
    name: Remove Old Branch Info
    runs-on: ubuntu-latest
    # Runs once at the beginning of each day or on manual trigger
    # Running on manual trigger effectively seeds the S3 bucket with all the necessary info
    if: ${{ github.event_name == 'workflow_dispatch' || github.event_name == 'schedule' }}

    steps:
    - name: Checkout
      uses: actions/checkout@v3
      with:
        fetch-depth: 0
        ref: main

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-1

    - name: Gather repository info
      run: |
        # Make clean directory that we can use to sync with S3
        mkdir ${WORK_DIR}

        # Get names of all remote branches and put in json array
        git branch -r | sed -e "s/  origin\///" | sed -e "s/HEAD -> origin\/main//" | jq -ncR '[ inputs | select(length > 0) ]' > ${WORK_DIR}/branches.json

        # Get tags and put in json array
        git tag | jq -ncR '[inputs]' > ${WORK_DIR}/tags.json

        process_files () {
          # Get sorted Topologies for branch
          find ./Environments/ -type f -name "*topo.*" -printf "%P\n" | jq -ncR "[inputs] | sort_by(.)" > $1/topo.json
          cat $1/topo.json

          # Get sorted Environments for branch
          find ./Environments/ -type f -name "environment.*" -printf "%P\n" | jq -ncR "[inputs] | sort_by(.)" > $1/env.json
          cat $1/env.json

          # Get list of Pytest dirs. Includes all dirs 4+ levels from root with the exception of "Resources" dirs
          pytest_dirs=$( find ./Tests/Pytest -mindepth 2 -type d \! -name "Resources" )
          # Get list of Pytest files
          pytest_files=$( find -type f -path './Tests/Pytest/*TestCases*' -name "test_*.py" )
          # Combine lists and sort
          printf "%s\n%s\n" $pytest_dirs $pytest_files | sed -e "s/^\.\///" | jq -ncR "[inputs | select(length > 0)]  | sort_by(.)" > $1/pytest_test_cases.json
          cat $1/pytest_test_cases.json

          # Get list of Robot dirs. Includes all dirs 4+ levels from root with the exception of "Resources" dirs
          robot_dirs=$( find ./Tests/Robot -mindepth 2 -type d \! -name "Resources" )
          # Get list of Robot files
          robot_files=$( find -type f -path './Tests/Robot/*TestCases*' -name "*.robot" )
          # Combine lists and sort
          printf "%s\n%s\n" $robot_dirs $robot_files | sed -e "s/^\.\///" | jq -ncR "[inputs | select(length > 0)]  | sort_by(.)" > $1/robot_test_cases.json
          cat $1/robot_test_cases.json
        }

        for branch in $(jq -r '.[]' ${WORK_DIR}/branches.json); do
          # Make directory for branch info
          echo "[*] Working on branch: ${branch}"
          branch_safe=$( echo ${branch} | sed -e "s/\///g") # Remove invalid filename chars before we use the branches to create folders
          branch_work_dir="${WORK_DIR}/${branch_safe}"
          mkdir ${branch_work_dir}

          # Checkout remote branch for inspection
          git checkout origin/${branch}

          process_files ${branch_work_dir}

        done

        for tag in $(jq -r '.[]' ${WORK_DIR}/tags.json); do
          # Make directory for tag info
          echo "[*] Working on tag: ${tag}"
          tag_safe=$( echo ${tag} | sed -e "s/\///g") # Remove invalid filename chars before we use the tags to create folders
          tag_work_dir="${WORK_DIR}/${tag_safe}"
          mkdir ${tag_work_dir}

          # Checkout remote branch for inspection
          git checkout tags/${tag}

          process_files ${tag_work_dir}

        done

        ls ${WORK_DIR}

    - name: Upload Metadata
      # Only delete stale branch folders. Exclude top level files and TestBeds directory
      run: |
        aws_sync_options=""
        if [[ "${{ github.event.inputs.dry-run }}" == "true" && "${{ github.event_name }}" == "workflow_dispatch" ]]; then
          aws_sync_options="--dryrun"
        fi

        aws s3 sync ${WORK_DIR} s3://econgit/extreme_automation_tests ${aws_sync_options} --delete --exclude "*" --include "*/*" --include "branches.json" --include "tags.json" --exclude "TestBeds/*"

  update_ref_lists:
    name: Update List of Branches/Tags
    runs-on: ubuntu-latest
    # Limit to one of these jobs at a time
    concurrency: s3_sync_ref_lists
    if: ${{ github.event_name == 'create' || github.event_name == 'delete' }}

    steps:
    - name: Checkout
      uses: actions/checkout@v3
      with:
        fetch-depth: 0
        ref: main

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-1

    - name: Gather repository info
      run: |
        # Make clean directory that we can use to sync with S3
        mkdir ${WORK_DIR}

        # Get names of all remote branches and put in json array
        git branch -r | sed -e "s/  origin\///" | sed -e "s/HEAD -> origin\/main//" | jq -ncR '[ inputs | select(length > 0) ]' > ${WORK_DIR}/branches.json

        # Get tags and put in json array
        git tag | jq -ncR '[inputs]' > ${WORK_DIR}/tags.json

    - name: Upload Metadata
      run: aws s3 sync ${WORK_DIR} s3://econgit/extreme_automation_tests

  update_ref_details:
    name: Update Branch/Tag Environments and Testcases
    runs-on: ubuntu-latest
    # Limit to one running job per branch
    concurrency: s3_sync_${{ github.ref_name }}
    if: ${{ github.event_name == 'push' }}

    steps:
    - name: Checkout
      uses: actions/checkout@v3
      with:
        ref: ${{ github.ref_name }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-1

    - name: Gather repository info
      run: |
        # Make clean directory that we can use to sync with S3
        mkdir ${WORK_DIR}

        # Make directory for branch/tag info
        echo "[*] Working on branch/tag: ${{ github.ref_name }}"
        ref_safe=$( echo ${{ github.ref_name }} | sed -e "s/\///g") # Remove invalid filename chars before we use the ref to create folders
        ref_work_dir="${WORK_DIR}/${ref_safe}"
        mkdir ${ref_work_dir}

        # Get sorted Topologies for branch/tag
        find ./Environments/ -type f -name "*topo.*" -printf "%P\n" | jq -ncR "[inputs] | sort_by(.)" > ${ref_work_dir}/topo.json
        cat ${ref_work_dir}/topo.json

        # Get sorted Environments for branch/tag
        find ./Environments/ -type f -name "environment.*" -printf "%P\n" | jq -ncR "[inputs] | sort_by(.)" > ${ref_work_dir}/env.json
        cat ${ref_work_dir}/env.json

        # Get list of Pytest dirs. Includes all dirs 4+ levels from root with the exception of "Resources" dirs
        pytest_dirs=$( find ./Tests/Pytest -mindepth 2 -type d \! -name "Resources" )
        # Get list of Pytest files
        pytest_files=$( find -type f -path './Tests/Pytest/*TestCases*' -name "test_*.py" )
        # Combine lists and sort
        printf "%s\n%s\n" $pytest_dirs $pytest_files | sed -e "s/^\.\///" | jq -ncR "[inputs | select(length > 0)]  | sort_by(.)" > ${ref_work_dir}/pytest_test_cases.json

        # Get list of Robot dirs. Includes all dirs 4+ levels from root with the exception of "Resources" dirs
        robot_dirs=$( find ./Tests/Robot -mindepth 2 -type d \! -name "Resources" )
        # Get list of Robot files
        robot_files=$( find -type f -path './Tests/Robot/*TestCases*' -name "*.robot" )
        # Combine lists and sort
        printf "%s\n%s\n" $robot_dirs $robot_files | sed -e "s/^\.\///" | jq -ncR "[inputs | select(length > 0)] | sort_by(.)" > ${ref_work_dir}/robot_test_cases.json

    - name: Upload Metadata
      run: aws s3 sync ${WORK_DIR} s3://econgit/extreme_automation_tests
